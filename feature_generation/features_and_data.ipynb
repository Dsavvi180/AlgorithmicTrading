{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c06a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a137de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-20 00:00:00+00:00</td>\n",
       "      <td>11712.986537</td>\n",
       "      <td>11716.011798</td>\n",
       "      <td>11711.611418</td>\n",
       "      <td>11712.986537</td>\n",
       "      <td>107.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-20 00:01:00+00:00</td>\n",
       "      <td>11713.261560</td>\n",
       "      <td>11713.261560</td>\n",
       "      <td>11708.861180</td>\n",
       "      <td>11710.786347</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-20 00:02:00+00:00</td>\n",
       "      <td>11710.236299</td>\n",
       "      <td>11713.811608</td>\n",
       "      <td>11709.961275</td>\n",
       "      <td>11713.811608</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-20 00:03:00+00:00</td>\n",
       "      <td>11712.986537</td>\n",
       "      <td>11713.261560</td>\n",
       "      <td>11710.236299</td>\n",
       "      <td>11710.511323</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-20 00:04:00+00:00</td>\n",
       "      <td>11710.511323</td>\n",
       "      <td>11710.786347</td>\n",
       "      <td>11709.411228</td>\n",
       "      <td>11710.511323</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741997</th>\n",
       "      <td>2025-07-17 23:55:00+00:00</td>\n",
       "      <td>23279.500000</td>\n",
       "      <td>23279.750000</td>\n",
       "      <td>23277.500000</td>\n",
       "      <td>23278.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741998</th>\n",
       "      <td>2025-07-17 23:56:00+00:00</td>\n",
       "      <td>23278.250000</td>\n",
       "      <td>23279.250000</td>\n",
       "      <td>23277.000000</td>\n",
       "      <td>23277.750000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741999</th>\n",
       "      <td>2025-07-17 23:57:00+00:00</td>\n",
       "      <td>23278.000000</td>\n",
       "      <td>23281.250000</td>\n",
       "      <td>23277.500000</td>\n",
       "      <td>23281.250000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742000</th>\n",
       "      <td>2025-07-17 23:58:00+00:00</td>\n",
       "      <td>23281.500000</td>\n",
       "      <td>23285.750000</td>\n",
       "      <td>23281.000000</td>\n",
       "      <td>23284.000000</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742001</th>\n",
       "      <td>2025-07-17 23:59:00+00:00</td>\n",
       "      <td>23283.500000</td>\n",
       "      <td>23285.500000</td>\n",
       "      <td>23280.500000</td>\n",
       "      <td>23280.750000</td>\n",
       "      <td>143.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1742002 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp          open          high           low  \\\n",
       "0        2020-07-20 00:00:00+00:00  11712.986537  11716.011798  11711.611418   \n",
       "1        2020-07-20 00:01:00+00:00  11713.261560  11713.261560  11708.861180   \n",
       "2        2020-07-20 00:02:00+00:00  11710.236299  11713.811608  11709.961275   \n",
       "3        2020-07-20 00:03:00+00:00  11712.986537  11713.261560  11710.236299   \n",
       "4        2020-07-20 00:04:00+00:00  11710.511323  11710.786347  11709.411228   \n",
       "...                            ...           ...           ...           ...   \n",
       "1741997  2025-07-17 23:55:00+00:00  23279.500000  23279.750000  23277.500000   \n",
       "1741998  2025-07-17 23:56:00+00:00  23278.250000  23279.250000  23277.000000   \n",
       "1741999  2025-07-17 23:57:00+00:00  23278.000000  23281.250000  23277.500000   \n",
       "1742000  2025-07-17 23:58:00+00:00  23281.500000  23285.750000  23281.000000   \n",
       "1742001  2025-07-17 23:59:00+00:00  23283.500000  23285.500000  23280.500000   \n",
       "\n",
       "                close  volume symbol  \n",
       "0        11712.986537   107.0   NQU0  \n",
       "1        11710.786347   112.0   NQU0  \n",
       "2        11713.811608    44.0   NQU0  \n",
       "3        11710.511323    59.0   NQU0  \n",
       "4        11710.511323    47.0   NQU0  \n",
       "...               ...     ...    ...  \n",
       "1741997  23278.000000    66.0   NQU5  \n",
       "1741998  23277.750000    62.0   NQU5  \n",
       "1741999  23281.250000    83.0   NQU5  \n",
       "1742000  23284.000000   115.0   NQU5  \n",
       "1742001  23280.750000   143.0   NQU5  \n",
       "\n",
       "[1742002 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('marketDataNasdaqFutures/NQ_continuous_backadjusted_1m_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff0e7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-20 00:00:00+00:00</th>\n",
       "      <td>11712.986537</td>\n",
       "      <td>11716.011798</td>\n",
       "      <td>11711.611418</td>\n",
       "      <td>11712.986537</td>\n",
       "      <td>107.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 00:01:00+00:00</th>\n",
       "      <td>11713.261560</td>\n",
       "      <td>11713.261560</td>\n",
       "      <td>11708.861180</td>\n",
       "      <td>11710.786347</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 00:02:00+00:00</th>\n",
       "      <td>11710.236299</td>\n",
       "      <td>11713.811608</td>\n",
       "      <td>11709.961275</td>\n",
       "      <td>11713.811608</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 00:03:00+00:00</th>\n",
       "      <td>11712.986537</td>\n",
       "      <td>11713.261560</td>\n",
       "      <td>11710.236299</td>\n",
       "      <td>11710.511323</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-20 00:04:00+00:00</th>\n",
       "      <td>11710.511323</td>\n",
       "      <td>11710.786347</td>\n",
       "      <td>11709.411228</td>\n",
       "      <td>11710.511323</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NQU0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-17 23:55:00+00:00</th>\n",
       "      <td>23279.500000</td>\n",
       "      <td>23279.750000</td>\n",
       "      <td>23277.500000</td>\n",
       "      <td>23278.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-17 23:56:00+00:00</th>\n",
       "      <td>23278.250000</td>\n",
       "      <td>23279.250000</td>\n",
       "      <td>23277.000000</td>\n",
       "      <td>23277.750000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-17 23:57:00+00:00</th>\n",
       "      <td>23278.000000</td>\n",
       "      <td>23281.250000</td>\n",
       "      <td>23277.500000</td>\n",
       "      <td>23281.250000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-17 23:58:00+00:00</th>\n",
       "      <td>23281.500000</td>\n",
       "      <td>23285.750000</td>\n",
       "      <td>23281.000000</td>\n",
       "      <td>23284.000000</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-17 23:59:00+00:00</th>\n",
       "      <td>23283.500000</td>\n",
       "      <td>23285.500000</td>\n",
       "      <td>23280.500000</td>\n",
       "      <td>23280.750000</td>\n",
       "      <td>143.0</td>\n",
       "      <td>NQU5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1691325 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   open          high           low  \\\n",
       "timestamp                                                             \n",
       "2020-07-20 00:00:00+00:00  11712.986537  11716.011798  11711.611418   \n",
       "2020-07-20 00:01:00+00:00  11713.261560  11713.261560  11708.861180   \n",
       "2020-07-20 00:02:00+00:00  11710.236299  11713.811608  11709.961275   \n",
       "2020-07-20 00:03:00+00:00  11712.986537  11713.261560  11710.236299   \n",
       "2020-07-20 00:04:00+00:00  11710.511323  11710.786347  11709.411228   \n",
       "...                                 ...           ...           ...   \n",
       "2025-07-17 23:55:00+00:00  23279.500000  23279.750000  23277.500000   \n",
       "2025-07-17 23:56:00+00:00  23278.250000  23279.250000  23277.000000   \n",
       "2025-07-17 23:57:00+00:00  23278.000000  23281.250000  23277.500000   \n",
       "2025-07-17 23:58:00+00:00  23281.500000  23285.750000  23281.000000   \n",
       "2025-07-17 23:59:00+00:00  23283.500000  23285.500000  23280.500000   \n",
       "\n",
       "                                  close  volume symbol  \n",
       "timestamp                                               \n",
       "2020-07-20 00:00:00+00:00  11712.986537   107.0   NQU0  \n",
       "2020-07-20 00:01:00+00:00  11710.786347   112.0   NQU0  \n",
       "2020-07-20 00:02:00+00:00  11713.811608    44.0   NQU0  \n",
       "2020-07-20 00:03:00+00:00  11710.511323    59.0   NQU0  \n",
       "2020-07-20 00:04:00+00:00  11710.511323    47.0   NQU0  \n",
       "...                                 ...     ...    ...  \n",
       "2025-07-17 23:55:00+00:00  23278.000000    66.0   NQU5  \n",
       "2025-07-17 23:56:00+00:00  23277.750000    62.0   NQU5  \n",
       "2025-07-17 23:57:00+00:00  23281.250000    83.0   NQU5  \n",
       "2025-07-17 23:58:00+00:00  23284.000000   115.0   NQU5  \n",
       "2025-07-17 23:59:00+00:00  23280.750000   143.0   NQU5  \n",
       "\n",
       "[1691325 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import time, datetime\n",
    "\n",
    "# =============== CONFIG ==================\n",
    "# Your raw data frame: df with columns: timestamp, open, high, low, close, volume\n",
    "# If your df is already loaded, start here:\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\") # Coerce converts non-numeric values to NaN\n",
    "df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "# ---------------- HOLIDAY LIST (ET full-day blocks) 2020‚Äì2025 ----------------\n",
    "# Notes:\n",
    "# - These are the standard US holidays where CME equity-index futures\n",
    "#   run a holiday schedule. We conservatively block the entire ET day.\n",
    "# - If you want precise early-close/late-open windows per year, replace a given\n",
    "#   date with an exact (start, end) window for that year.\n",
    "# - Source references: CME trading-hours portal and CME holiday/clearing advisories. \n",
    "#   (We block: New Year‚Äôs Day (observed), MLK, Presidents, Good Friday, Memorial,\n",
    "#    Juneteenth (from 2022), Independence Day (observed), Labor, Thanksgiving, Christmas (observed).)\n",
    "\n",
    "HOLIDAY_DATES_ET = [\n",
    "    # ----- 2020 -----\n",
    "    \"2020-01-01\",  # New Year‚Äôs Day\n",
    "    \"2020-01-20\",  # MLK Day\n",
    "    \"2020-02-17\",  # Presidents Day\n",
    "    \"2020-04-10\",  # Good Friday\n",
    "    \"2020-05-25\",  # Memorial Day\n",
    "    \"2020-07-03\",  # Independence Day (observed)\n",
    "    \"2020-09-07\",  # Labor Day\n",
    "    \"2020-11-26\",  # Thanksgiving Day\n",
    "    \"2020-11-27\",  # Day after Thanksgiving <- ADDED\n",
    "    \"2020-12-24\",  # Christmas Eve <- ADDED\n",
    "    \"2020-12-25\",  # Christmas Day\n",
    "\n",
    "    # ----- 2021 -----\n",
    "    \"2021-01-01\",  # New Year‚Äôs Day\n",
    "    \"2021-01-18\",  # MLK Day\n",
    "    \"2021-02-15\",  # Presidents Day\n",
    "    \"2021-04-02\",  # Good Friday\n",
    "    \"2021-05-31\",  # Memorial Day\n",
    "    \"2021-07-05\",  # Independence Day (observed)\n",
    "    \"2021-09-06\",  # Labor Day\n",
    "    \"2021-11-25\",  # Thanksgiving Day\n",
    "    \"2021-11-26\",  # Day after Thanksgiving <- ADDED\n",
    "    \"2021-12-24\",  # Christmas Day (observed)\n",
    "\n",
    "    # ----- 2022 -----\n",
    "    # \"2022-01-01\" is Saturday, observed Mon Jan 3 (Stock market open, NQ futures normal)\n",
    "    # Your list correctly omits Jan 1 and Jan 3, 2022.\n",
    "    \"2022-01-17\",  # MLK Day\n",
    "    \"2022-02-21\",  # Presidents Day\n",
    "    \"2022-04-15\",  # Good Friday\n",
    "    \"2022-05-30\",  # Memorial Day\n",
    "    \"2022-06-20\",  # Juneteenth (observed)\n",
    "    \"2022-07-04\",  # Independence Day\n",
    "    \"2022-09-05\",  # Labor Day\n",
    "    \"2022-11-24\",  # Thanksgiving Day\n",
    "    \"2022-11-25\",  # Day after Thanksgiving <- ADDED\n",
    "    \"2022-12-26\",  # Christmas Day (observed)\n",
    "\n",
    "    # ----- 2023 -----\n",
    "    \"2023-01-02\",  # New Year‚Äôs Day (observed)\n",
    "    \"2023-01-16\",  # MLK Day\n",
    "    \"2023-02-20\",  # Presidents Day\n",
    "    \"2023-04-07\",  # Good Friday\n",
    "    \"2023-05-29\",  # Memorial Day\n",
    "    \"2023-06-19\",  # Juneteenth\n",
    "    \"2023-07-03\",  # Day before Independence Day <- ADDED\n",
    "    \"2023-07-04\",  # Independence Day\n",
    "    \"2023-09-04\",  # Labor Day\n",
    "    \"2023-11-23\",  # Thanksgiving Day\n",
    "    \"2023-11-24\",  # Day after Thanksgiving <- ADDED\n",
    "    \"2023-12-25\",  # Christmas Day\n",
    "\n",
    "    # ----- 2024 -----\n",
    "    \"2024-01-01\",  # New Year‚Äôs Day\n",
    "    \"2024-01-15\",  # MLK Day\n",
    "    \"2024-02-19\",  # Presidents Day\n",
    "    \"2024-03-29\",  # Good Friday\n",
    "    \"2024-05-27\",  # Memorial Day\n",
    "    \"2024-06-19\",  # Juneteenth\n",
    "    \"2024-07-03\",  # Day before Independence Day <- ADDED\n",
    "    \"2024-07-04\",  # Independence Day\n",
    "    \"2024-09-02\",  # Labor Day\n",
    "    \"2024-11-28\",  # Thanksgiving Day\n",
    "    \"2024-11-29\",  # Day after Thanksgiving <- ADDED\n",
    "    \"2024-12-24\",  # Christmas Eve <- ADDED\n",
    "    \"2024-12-25\",  # Christmas Day\n",
    "\n",
    "    # ----- 2025 -----\n",
    "    \"2025-01-01\",  # New Year‚Äôs Day\n",
    "    \"2025-01-20\",  # MLK Day\n",
    "    \"2025-02-17\",  # Presidents Day\n",
    "    \"2025-04-18\",  # Good Friday\n",
    "    \"2025-05-26\",  # Memorial Day\n",
    "    \"2025-06-19\",  # Juneteenth\n",
    "    \"2025-07-03\",  # Day before Independence Day <- ADDED\n",
    "    \"2025-07-04\",  # Independence Day\n",
    "    \"2025-09-01\",  # Labor Day\n",
    "    \"2025-11-27\",  # Thanksgiving Day\n",
    "    \"2025-11-28\",  # Day after Thanksgiving <- ADDED\n",
    "    \"2025-12-24\",  # Christmas Eve <- ADDED\n",
    "    \"2025-12-25\",  # Christmas Day\n",
    "]\n",
    "\n",
    "# Build concrete (start,end) ET windows for each holiday date (full day)\n",
    "HOLIDAY_WINDOWS_ET = [\n",
    "    (\n",
    "        pd.Timestamp(d + \" 00:00\", tz=\"America/New_York\"),\n",
    "        pd.Timestamp(d + \" 23:59:59.999999\", tz=\"America/New_York\"),\n",
    "    )\n",
    "    for d in HOLIDAY_DATES_ET\n",
    "]\n",
    "\n",
    "# =============== FILTER LOGIC ==================\n",
    "# Convert index to ET for session logic\n",
    "et_index = df.index.tz_convert(\"America/New_York\")\n",
    "et_time = et_index.time\n",
    "et_wday = et_index.weekday  # Monday=0 ... Sunday=6\n",
    "\n",
    "# Daily Globex maintenance break (NOT tradable): 17:00‚Äì18:00 ET\n",
    "not_maint = ~((et_time >= time(17, 0)) & (et_time < time(18, 0)))\n",
    "\n",
    "# Weekly structure:\n",
    "# Mon..Thu: tradable except maintenance\n",
    "mon_thu = (et_wday >= 0) & (et_wday <= 3)\n",
    "\n",
    "# Friday: tradable only before 17:00 ET\n",
    "fri = (et_wday == 4) & (et_time < time(17, 0))\n",
    "\n",
    "# Sunday: tradable only from 18:00 ET onward\n",
    "sun = (et_wday == 6) & (et_time >= time(18, 0))\n",
    "\n",
    "# Saturday: closed (no hours)\n",
    "# (implicitly excluded; nothing to include on Saturday)\n",
    "weekly_open = mon_thu | fri | sun\n",
    "\n",
    "standard_tradable = weekly_open & not_maint\n",
    "\n",
    "# Holidays overlay (drop anything inside these windows)\n",
    "holiday_mask = pd.Series(True, index=df.index)\n",
    "if HOLIDAY_WINDOWS_ET:\n",
    "    keep = pd.Series(True, index=df.index)\n",
    "    et_ts = et_index\n",
    "    for start_et, end_et in HOLIDAY_WINDOWS_ET:\n",
    "        keep &= ~((et_ts >= start_et) & (et_ts <= end_et)) # Negate truth values in keep that meet the holiday condition\n",
    "    holiday_mask = keep\n",
    "\n",
    "# Final filtered DataFrame (UTC index preserved)\n",
    "df_filtered = df[standard_tradable & holiday_mask].copy()\n",
    "\n",
    "# Optional: if you‚Äôd like an ET-indexed version for inspection:\n",
    "# df_filtered_et = df_filtered.copy()\n",
    "# df_filtered_et.index = df_filtered_et.index.tz_convert(\"America/New_York\")\n",
    "df_filtered.ffill(inplace=True)\n",
    "df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build only the z-scored OHLC columns, with past-only info\n",
    "df_norm = pd.DataFrame(index=df_filtered.index)\n",
    "for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "    s  = df_filtered[col].astype(float)\n",
    "    m  = s.rolling(100, min_periods=1).mean()\n",
    "    sd = s.rolling(100, min_periods=1).std()\n",
    "    z  = (s - m) / sd.replace(0, np.nan)\n",
    "    df_norm[f\"{col}_z100\"] = z.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "df_norm_zscore = df_norm  # same columns as before, no future backfill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fda34c",
   "metadata": {},
   "source": [
    "# üìä Feature Documentation for Market ML Model\n",
    "\n",
    "This document explains all engineered features generated from the OHLCV time-series.  \n",
    "All features are computed in **UTC** and are **lagged to avoid look-ahead bias**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Core Principles\n",
    "\n",
    "- **All inputs remain in UTC**  \n",
    "- **No leakage:** Any rolling statistic uses `.shift(1)` to ensure only past data is used  \n",
    "- **Symbols supported:** Features can be computed for one or multiple tickers  \n",
    "- **Returns computed on log prices**  \n",
    "- **Three return variants included:** raw, rolling-z, and volatility-scaled  \n",
    "\n",
    "---\n",
    "\n",
    "## üß† Feature Categories\n",
    "\n",
    "The features fall into several groups:\n",
    "\n",
    "1. **Returns & Return Normalisations**\n",
    "2. **Volatility & Range Measures**\n",
    "3. **Momentum / Trend**\n",
    "4. **Volume & Flow**\n",
    "5. **VWAP & Fair-Value Distance**\n",
    "6. **Microstructure Proxies (from OHLCV)**\n",
    "7. **Seasonality & Time Encoding**\n",
    "8. **GARCH Model-Based Volatility**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. üîÅ Returns & Return Normalisations\n",
    "\n",
    "| Feature | Description | Notes |\n",
    "|---------|---------------|--------|\n",
    "| `ret_1` | 1-bar log return | Lagged by 1 bar |\n",
    "| `ret_1_z` | Rolling z-score of 1-bar return | Uses past mean & std to normalise |\n",
    "| `ret_1_volsc` | 1-bar return scaled by ‚àö(rolling variance) | Makes returns stationary across regimes |\n",
    "| `ret_k` | k-bar cumulative log return | `'k'` in config; lagged |\n",
    "| `ret_k_z` | Rolling z-score of k-bar return | Normalised cumulative return |\n",
    "| `ret_k_volsc` | k-bar return scaled by rolling variance | Volatility-adjusted |\n",
    "\n",
    "<font color=\"green\" >These help the model learn return magnitude, normalised behaviour, and regime-adjusted movement. </font>\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üìà Volatility & Range Measures\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|--------------|\n",
    "| `atr` | Average True Range (Wilder‚Äôs) ‚Äì measures price range volatility |\n",
    "| `parkinson_var` | Parkinson range-based variance using ln(high/low)¬≤ |\n",
    "| `rv` | Realised volatility: ‚àö(Œ£ returns¬≤ over window) |\n",
    "| `vol_of_vol` | Volatility of volatility (stdev of return stdev) |\n",
    "\n",
    "<font color=\"green\" >Captures short-term, range-based, and realised price volatility and volatility regime shifts.</font>\n",
    "\n",
    "---\n",
    "\n",
    "## 3. üìâ Momentum / Trend Indicators\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|--------------|\n",
    "| `ema_fast` | Fast EMA of close (default 12) |\n",
    "| `ema_slow` | Slow EMA of close (default 26) |\n",
    "| `macd` | Fast EMA ‚Äì Slow EMA (trend momentum) |\n",
    "| `macd_sig` | EMA of MACD (signal line) |\n",
    "| `macd_div` | MACD ‚Äì Signal (histogram; trend strength) |\n",
    "| `ema_spread` | Close minus fast EMA (distance to trend anchor) |\n",
    "\n",
    "<font color=\"green\">These measure trending behavior, trend acceleration, and trend strength.</font>\n",
    "\n",
    "---\n",
    "\n",
    "## 4. üì¶ Volume & Flow Indicators\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|--------------|\n",
    "| `vol_z` | Volume z-score (deviation from rolling norm) |\n",
    "| `obv` | On-Balance Volume ‚Äì cumulative volume adjusted by direction |\n",
    "| `cmf` | Chaikin Money Flow ‚Äì volume-weighted buying/selling pressure |\n",
    "\n",
    "\n",
    "<font color=\"green\">Captures demand/supply imbalance, momentum supported by volume, and accumulation/distribution phases.</font>\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ‚öñÔ∏è VWAP & Fair-Value Distance\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|--------------|\n",
    "| `vwap_roll` | Rolling VWAP (typical price √ó volume / sum volume) |\n",
    "| `dist_vwap` | Close ‚Äì VWAP (raw deviation) |\n",
    "| `dist_vwap_z` | Distance to VWAP scaled by ATR (vol-adjusted fair-value gap) |\n",
    "\n",
    "<font color=\"green\">Captures demand/supply imbalance, momentum supported by volume, and accumulation/distribution phases.</font>\n",
    "\n",
    "---\n",
    "\n",
    "## 6. üß© Microstructure Proxies (from OHLCV Only)\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|--------------|\n",
    "| `clv` | Close Location Value: (Close‚ÄìLow)/(High‚ÄìLow) ‚àà [0,1] |\n",
    "| `gap_pct` | Gap size between prev close and open (absolute % gap) |\n",
    "| `dir_persist` | Fraction of positive returns in rolling window |\n",
    "\n",
    "<font color=\"green\">These approximate order-flow or market microstructure signals without needing Level-2 data.</font>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. üïí Seasonality & Time Encoding (UTC)\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|--------------|\n",
    "| `tod_sin`, `tod_cos` | Time-of-day encoded as cyclical features |\n",
    "| `dow_sin`, `dow_cos` | Day-of-week encoded cyclically |\n",
    "\n",
    "\n",
    "<font color=\"green\">Helps the model identify behaviour that depends on time of day or day of week.</font>\n",
    "\n",
    "---\n",
    "\n",
    "## 8. üìâ GARCH Model-Based Volatility\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|--------------|\n",
    "| `garch_sigma` | Conditional volatility (œÉ‚Çú) from best-fit GARCH model |\n",
    "\n",
    "### How it‚Äôs computed:\n",
    "\n",
    "- A small grid search is run on training data over:  \n",
    "  `(p,q) ‚àà {(1,1), (1,2), (2,1)}` √ó `{Normal, Student-T}`\n",
    "- Best model is selected using AIC (or BIC if configured)\n",
    "- Model is **not refit on test** ‚Äì parameters are fixed and the full series is filtered\n",
    "- œÉ‚Çú is lagged by 1 bar to avoid leakage\n",
    "\n",
    "\n",
    "<font color=\"green\">Adds a robust statistical volatility estimate that adapts to volatility clustering.</font>\n",
    "\n",
    "---\n",
    "## 9. Volume Profile features \n",
    "- volprofile library https://pypi.org/project/volprofile/\n",
    "\n",
    "---\n",
    "## 10. Technical Analysis features\n",
    "- Pattern mining using Ta-lib\n",
    "- double-top, maxima/minima, head & shoulders etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üßº Cleaning & Safety\n",
    "\n",
    "- All time-series windows use **`.shift(1)`** to avoid forward-looking leakage\n",
    "- Infinite and NaN values dropped after feature assembly\n",
    "- Index remains in **UTC at all times**\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇ Returned Output Format\n",
    "\n",
    "- A DataFrame with all features + a `symbol` column\n",
    "- A dictionary of best GARCH specs per symbol, e.g.:\n",
    "\n",
    "\n",
    "## üß† Feature reduction\n",
    "- SHAP for feature importance filtering\n",
    "- ANOVA F-tests for seeing which models have a significant impact on explaining variance in the response\n",
    "- Correlation Pruning: remove features >0.9 correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5244ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_length = len(df_filtered)\n",
    "prices = df_filtered['close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb815b8",
   "metadata": {},
   "source": [
    "### Returns & Return variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02a1e3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_returns_variants created with shape: (1691325, 74)\n"
     ]
    }
   ],
   "source": [
    "# Returns & Return Normalisation\n",
    "# ===============================\n",
    "\n",
    "# 1 bar regular returns\n",
    "# ------------------------------\n",
    "returns = prices.pct_change()\n",
    "# Remove returns that jump across >2min gap\n",
    "returns = returns.where(returns.index.to_series().diff() <= pd.Timedelta(\"2min\"), 0.0)\n",
    "returns = returns.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "# Z-score normalisation of regular returns (X-Mew)/SD over multiple windows\n",
    "# ------------------------------\n",
    "def zscore(series, window):\n",
    "    rolling_mean = series.rolling(window, min_periods=1).mean()\n",
    "    rolling_std = series.rolling(window, min_periods=1).std()\n",
    "    return (series - rolling_mean) / rolling_std.replace(0, np.nan)\n",
    "\n",
    "returns_z_20   = zscore(returns, 20)\n",
    "returns_z_60   = zscore(returns, 60)\n",
    "returns_z_120  = zscore(returns, 120)\n",
    "returns_z_240  = zscore(returns, 240)\n",
    "returns_z_360  = zscore(returns, 360)\n",
    "returns_z_720  = zscore(returns, 720)\n",
    "\n",
    "# Volatility adjusted returns \n",
    "# ------------------------------\n",
    "def vol_scaled(series, window):\n",
    "    rolling_std = series.rolling(window, min_periods=1).std()\n",
    "    return (series / rolling_std.replace(0, np.nan))\n",
    "\n",
    "returns_vol_20   = vol_scaled(returns, 20)\n",
    "returns_vol_60   = vol_scaled(returns, 60)\n",
    "returns_vol_120  = vol_scaled(returns, 120)\n",
    "returns_vol_240  = vol_scaled(returns, 240)\n",
    "returns_vol_360  = vol_scaled(returns, 360)\n",
    "returns_vol_720  = vol_scaled(returns, 720)\n",
    "\n",
    "# Log returns log(p_t) - log(p_{t-1})\n",
    "# ------------------------------\n",
    "log_returns = np.log(prices) - np.log(prices.shift(1))\n",
    "log_returns = log_returns.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "# Remove returns that jump across >2min gap (assuming 1-min bars), prevents return spikes between sessions/trading and non-trading days\n",
    "log_returns = log_returns.where(log_returns.index.to_series().diff() <= pd.Timedelta(\"2min\"), 0.0)\n",
    "log_returns_vol_20   = vol_scaled(log_returns, 20)\n",
    "log_returns_vol_60   = vol_scaled(log_returns, 60)\n",
    "log_returns_vol_120  = vol_scaled(log_returns, 120)\n",
    "log_returns_vol_240  = vol_scaled(log_returns, 240) \n",
    "log_returns_vol_360  = vol_scaled(log_returns, 360)\n",
    "log_returns_vol_720  = vol_scaled(log_returns, 720)\n",
    "\n",
    "#Z-score normalisation of log returns (X-Mew)/SD over multiple windows\n",
    "# ------------------------------\n",
    "log_returns_z_20   = zscore(log_returns, 20)\n",
    "log_returns_z_60   = zscore(log_returns, 60)\n",
    "log_returns_z_120  = zscore(log_returns, 120)\n",
    "log_returns_z_240  = zscore(log_returns, 240)\n",
    "log_returns_z_360  = zscore(log_returns, 360)\n",
    "log_returns_z_720  = zscore(log_returns, 720)\n",
    "\n",
    "# k- bar returns\n",
    "# ------------------------------\n",
    "def kbar_simple_return(prices, k):\n",
    "    return (prices.shift(-k) / prices) - 1.0\n",
    "returns_k_5   = kbar_simple_return(prices, 5)\n",
    "returns_k_15  = kbar_simple_return(prices, 15)\n",
    "returns_k_30  = kbar_simple_return(prices, 30)\n",
    "returns_k_60  = kbar_simple_return(prices, 60)\n",
    "returns_k_120 = kbar_simple_return(prices, 120)\n",
    "returns_k_240 = kbar_simple_return(prices, 240)\n",
    "returns_k_360 = kbar_simple_return(prices, 360)\n",
    "returns_k_720 = kbar_simple_return(prices, 720)\n",
    "\n",
    "# k-bar log returns\n",
    "# ------------------------------\n",
    "def kbar_log_return(log_returns, k):\n",
    "    # Gap in time series can cause large return spikes; we handle this below\n",
    "    # --------------------------------\n",
    "    # Detect any gap > 2 min within the future horizon (t, t+k]\n",
    "    gap_now = prices.index.to_series().diff() > pd.Timedelta(\"2min\")\n",
    "    gap_in_future = gap_now.rolling(window=k).max().shift(-k).fillna(0).astype(bool)\n",
    "\n",
    "    # Apply to simple and log k-bar labels\n",
    "    def safe_numeric(s):\n",
    "        return s.where(~gap_in_future, 0.0).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    return safe_numeric(log_returns.rolling(k).sum().shift(-k))\n",
    "\n",
    "log_returns_k_5   = kbar_log_return(log_returns, 5)\n",
    "log_returns_k_15  = kbar_log_return(log_returns, 15)\n",
    "log_returns_k_30  = kbar_log_return(log_returns, 30)\n",
    "log_returns_k_60  = kbar_log_return(log_returns, 60)\n",
    "log_returns_k_120 = kbar_log_return(log_returns, 120)\n",
    "log_returns_k_240 = kbar_log_return(log_returns, 240)       \n",
    "log_returns_k_360 = kbar_log_return(log_returns, 360)\n",
    "log_returns_k_720 = kbar_log_return(log_returns, 720)\n",
    "\n",
    "# k-bar z_score returns\n",
    "# ------------------------------\n",
    "returns_k_z_5   = zscore(returns_k_5, 20)\n",
    "returns_k_z_15  = zscore(returns_k_15, 60)\n",
    "returns_k_z_30  = zscore(returns_k_30, 120)\n",
    "returns_k_z_60  = zscore(returns_k_60, 240)\n",
    "returns_k_z_120 = zscore(returns_k_120, 360)\n",
    "returns_k_z_240 = zscore(returns_k_240, 720)\n",
    "returns_k_z_360 = zscore(returns_k_360, 1440)\n",
    "returns_k_z_720 = zscore(returns_k_720, 2880)\n",
    "\n",
    "# k-bar log z_score returns\n",
    "# ------------------------------\n",
    "log_returns_k_z_5   = zscore(log_returns_k_5, 20)\n",
    "log_returns_k_z_15  = zscore(log_returns_k_15, 60)\n",
    "log_returns_k_z_30  = zscore(log_returns_k_30, 120)\n",
    "log_returns_k_z_60  = zscore(log_returns_k_60, 240)\n",
    "log_returns_k_z_120 = zscore(log_returns_k_120, 360)\n",
    "log_returns_k_z_240 = zscore(log_returns_k_240, 720)\n",
    "log_returns_k_z_360 = zscore(log_returns_k_360, 1440)\n",
    "log_returns_k_z_720 = zscore(log_returns_k_720, 2880)\n",
    "\n",
    "# k-bar volatility scaled log returns\n",
    "# ------------------------------\n",
    "log_returns_k_vol_5   = vol_scaled(log_returns_k_5, 20)\n",
    "log_returns_k_vol_15  = vol_scaled(log_returns_k_15, 60)\n",
    "log_returns_k_vol_30  = vol_scaled(log_returns_k_30, 120)\n",
    "log_returns_k_vol_60  = vol_scaled(log_returns_k_60, 240)\n",
    "log_returns_k_vol_120 = vol_scaled(log_returns_k_120, 360)\n",
    "log_returns_k_vol_240 = vol_scaled(log_returns_k_240, 720)\n",
    "log_returns_k_vol_360 = vol_scaled(log_returns_k_360, 1440)\n",
    "log_returns_k_vol_720 = vol_scaled(log_returns_k_720, 2880)\n",
    "\n",
    "# k-bar volatility scaled returns\n",
    "# ------------------------------\n",
    "returns_k_vol_5   = vol_scaled(returns_k_5, 20)\n",
    "returns_k_vol_15  = vol_scaled(returns_k_15, 60)\n",
    "returns_k_vol_30  = vol_scaled(returns_k_30, 120)               \n",
    "returns_k_vol_60  = vol_scaled(returns_k_60, 240)\n",
    "returns_k_vol_120 = vol_scaled(returns_k_120, 360)\n",
    "returns_k_vol_240 = vol_scaled(returns_k_240, 720)\n",
    "returns_k_vol_360 = vol_scaled(returns_k_360, 1440)\n",
    "returns_k_vol_720 = vol_scaled(returns_k_720, 2880) \n",
    "\n",
    "# ================== BUILD df_returns_variants ==================\n",
    "\n",
    "df_returns_variants = pd.DataFrame(index=df_filtered.index)\n",
    "\n",
    "# 1-bar returns\n",
    "df_returns_variants[\"ret_1\"] = returns\n",
    "df_returns_variants[\"ret_1_log\"] = log_returns\n",
    "\n",
    "# 1-bar Z-scores\n",
    "df_returns_variants[\"ret_1_z20\"] = returns_z_20\n",
    "df_returns_variants[\"ret_1_z60\"] = returns_z_60\n",
    "df_returns_variants[\"ret_1_z120\"] = returns_z_120\n",
    "df_returns_variants[\"ret_1_z240\"] = returns_z_240\n",
    "df_returns_variants[\"ret_1_z360\"] = returns_z_360\n",
    "df_returns_variants[\"ret_1_z720\"] = returns_z_720\n",
    "\n",
    "# 1-bar volatility scaled\n",
    "df_returns_variants[\"ret_1_vol20\"] = returns_vol_20\n",
    "df_returns_variants[\"ret_1_vol60\"] = returns_vol_60\n",
    "df_returns_variants[\"ret_1_vol120\"] = returns_vol_120\n",
    "df_returns_variants[\"ret_1_vol240\"] = returns_vol_240\n",
    "df_returns_variants[\"ret_1_vol360\"] = returns_vol_360\n",
    "df_returns_variants[\"ret_1_vol720\"] = returns_vol_720\n",
    "\n",
    "\n",
    "# 1-bar log volatility scaled\n",
    "df_returns_variants[\"ret_1_log_vol20\"] = log_returns_vol_20\n",
    "df_returns_variants[\"ret_1_log_vol60\"] = log_returns_vol_60\n",
    "df_returns_variants[\"ret_1_log_vol120\"] = log_returns_vol_120\n",
    "df_returns_variants[\"ret_1_log_vol240\"] = log_returns_vol_240\n",
    "df_returns_variants[\"ret_1_log_vol360\"] = log_returns_vol_360\n",
    "df_returns_variants[\"ret_1_log_vol720\"] = log_returns_vol_720\n",
    "\n",
    "\n",
    "# 1-bar log Z-scores\n",
    "df_returns_variants[\"ret_1_log_z20\"] = log_returns_z_20\n",
    "df_returns_variants[\"ret_1_log_z60\"] = log_returns_z_60\n",
    "df_returns_variants[\"ret_1_log_z120\"] = log_returns_z_120\n",
    "df_returns_variants[\"ret_1_log_z240\"] = log_returns_z_240\n",
    "df_returns_variants[\"ret_1_log_z360\"] = log_returns_z_360\n",
    "df_returns_variants[\"ret_1_log_z720\"] = log_returns_z_720\n",
    "\n",
    "\n",
    "# k-bar simple returns\n",
    "df_returns_variants[\"ret_k5\"] = returns_k_5\n",
    "df_returns_variants[\"ret_k15\"] = returns_k_15\n",
    "df_returns_variants[\"ret_k30\"] = returns_k_30\n",
    "df_returns_variants[\"ret_k60\"] = returns_k_60\n",
    "df_returns_variants[\"ret_k120\"] = returns_k_120\n",
    "df_returns_variants[\"ret_k240\"] = returns_k_240\n",
    "df_returns_variants[\"ret_k360\"] = returns_k_360\n",
    "df_returns_variants[\"ret_k720\"] = returns_k_720\n",
    "\n",
    "\n",
    "# k-bar log returns\n",
    "df_returns_variants[\"ret_k5_log\"] = log_returns_k_5\n",
    "df_returns_variants[\"ret_k15_log\"] = log_returns_k_15\n",
    "df_returns_variants[\"ret_k30_log\"] = log_returns_k_30\n",
    "df_returns_variants[\"ret_k60_log\"] = log_returns_k_60\n",
    "df_returns_variants[\"ret_k120_log\"] = log_returns_k_120\n",
    "df_returns_variants[\"ret_k240_log\"] = log_returns_k_240\n",
    "df_returns_variants[\"ret_k360_log\"] = log_returns_k_360\n",
    "df_returns_variants[\"ret_k720_log\"] = log_returns_k_720\n",
    "\n",
    "\n",
    "# k-bar volatility scaled returns\n",
    "df_returns_variants[\"ret_k5_vol\"] = returns_k_vol_5\n",
    "df_returns_variants[\"ret_k15_vol\"] = returns_k_vol_15\n",
    "df_returns_variants[\"ret_k30_vol\"] = returns_k_vol_30\n",
    "df_returns_variants[\"ret_k60_vol\"] = returns_k_vol_60\n",
    "df_returns_variants[\"ret_k120_vol\"] = returns_k_vol_120\n",
    "df_returns_variants[\"ret_k240_vol\"] = returns_k_vol_240\n",
    "df_returns_variants[\"ret_k360_vol\"] = returns_k_vol_360\n",
    "df_returns_variants[\"ret_k720_vol\"] = returns_k_vol_720\n",
    "\n",
    "\n",
    "# k-bar log volatility scaled returns\n",
    "df_returns_variants[\"ret_k5_log_vol\"] = log_returns_k_vol_5\n",
    "df_returns_variants[\"ret_k15_log_vol\"] = log_returns_k_vol_15\n",
    "df_returns_variants[\"ret_k30_log_vol\"] = log_returns_k_vol_30\n",
    "df_returns_variants[\"ret_k60_log_vol\"] = log_returns_k_vol_60\n",
    "df_returns_variants[\"ret_k120_log_vol\"] = log_returns_k_vol_120\n",
    "df_returns_variants[\"ret_k240_log_vol\"] = log_returns_k_vol_240\n",
    "df_returns_variants[\"ret_k360_log_vol\"] = log_returns_k_vol_360\n",
    "df_returns_variants[\"ret_k720_log_vol\"] = log_returns_k_vol_720\n",
    "\n",
    "\n",
    "# k-bar Z-scores\n",
    "df_returns_variants[\"ret_k5_z\"] = returns_k_z_5\n",
    "df_returns_variants[\"ret_k15_z\"] = returns_k_z_15\n",
    "df_returns_variants[\"ret_k30_z\"] = returns_k_z_30\n",
    "df_returns_variants[\"ret_k60_z\"] = returns_k_z_60\n",
    "df_returns_variants[\"ret_k120_z\"] = returns_k_z_120\n",
    "df_returns_variants[\"ret_k240_z\"] = returns_k_z_240\n",
    "df_returns_variants[\"ret_k360_z\"] = returns_k_z_360\n",
    "df_returns_variants[\"ret_k720_z\"] = returns_k_z_720\n",
    "\n",
    "\n",
    "# k-bar log Z-scores\n",
    "df_returns_variants[\"ret_k5_log_z\"] = log_returns_k_z_5\n",
    "df_returns_variants[\"ret_k15_log_z\"] = log_returns_k_z_15\n",
    "df_returns_variants[\"ret_k30_log_z\"] = log_returns_k_z_30\n",
    "df_returns_variants[\"ret_k60_log_z\"] = log_returns_k_z_60\n",
    "df_returns_variants[\"ret_k120_log_z\"] = log_returns_k_z_120\n",
    "df_returns_variants[\"ret_k240_log_z\"] = log_returns_k_z_240\n",
    "df_returns_variants[\"ret_k360_log_z\"] = log_returns_k_z_360\n",
    "df_returns_variants[\"ret_k720_log_z\"] = log_returns_k_z_720\n",
    "\n",
    "\n",
    "# Final sanity check\n",
    "assert len(df_returns_variants) == df_length\n",
    "print(\"df_returns_variants created with shape:\", df_returns_variants.shape)\n",
    "df_returns_variants.replace(np.nan, 0, inplace=True)\n",
    "NaN_col =df_returns_variants.isna().sum()\n",
    "NaN_col =df_returns_variants.isna().sum()\n",
    "\n",
    "# ================END OF CODE==================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f189b194",
   "metadata": {},
   "source": [
    "### Range and Volatility Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7becde7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_volatility_features created with shape: (1691325, 168)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "atr_tr                           0\n",
       "atr_14_rma                       0\n",
       "atr_14_rma_pct                   0\n",
       "atr_sma_14_sma                   0\n",
       "atr_sma_14_sma_pct               0\n",
       "                                ..\n",
       "vov_realised_vol_380_log_240     0\n",
       "vov_realised_vol_1380_60         0\n",
       "vov_realised_vol_1380_log_60     0\n",
       "vov_realised_vol_1380_240        0\n",
       "vov_realised_vol_1380_log_240    0\n",
       "Length: 168, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def ensure_reindex(df: pd.DataFrame, index: pd.Index) -> pd.DataFrame:\n",
    "    \"\"\"Align a frame/series to a canonical index (no forward/back fill).\"\"\"\n",
    "    return df.reindex(index)\n",
    "\n",
    "def downcast_floats(df: pd.DataFrame, float_dtype=\"float32\") -> pd.DataFrame:\n",
    "    \"\"\"Downcast float columns to save RAM.\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if pd.api.types.is_float_dtype(out[c]):\n",
    "            out[c] = out[c].astype(float_dtype)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# ATR (Average True Range)\n",
    "# -----------------------------\n",
    "def add_atr(\n",
    "    df: pd.DataFrame,\n",
    "    n: int = 14,\n",
    "    method: str = \"rma\",     # \"rma\" (Wilder) | \"sma\"\n",
    "    pct: bool = True,        # add ATR% column (ATR/close)\n",
    "    gap_safe: bool = True,   # avoid huge TR across big gaps\n",
    "    gap_timedelta: str = \"2min\",\n",
    "    hi_col: str = \"high\",\n",
    "    lo_col: str = \"low\",\n",
    "    cl_col: str = \"close\",\n",
    "    out_prefix: str = \"atr\",\n",
    "    include_tr: bool = False # create *_tr once to avoid duplicate col names\n",
    ") -> pd.DataFrame:\n",
    "    assert {hi_col, lo_col, cl_col}.issubset(df.columns), \"Missing price columns\"\n",
    "    high  = df[hi_col].astype(float)\n",
    "    low   = df[lo_col].astype(float)\n",
    "    close = df[cl_col].astype(float)\n",
    "\n",
    "    prev_close = close.shift(1)\n",
    "    if gap_safe:\n",
    "        gap_mask = df.index.to_series().diff() > pd.Timedelta(gap_timedelta)\n",
    "        prev_close = prev_close.where(~gap_mask, close)\n",
    "\n",
    "    hl = (high - low).abs()\n",
    "    hc = (high - prev_close).abs()\n",
    "    lc = (low  - prev_close).abs()\n",
    "    tr = pd.concat([hl, hc, lc], axis=1).max(axis=1)\n",
    "\n",
    "    method = method.lower()\n",
    "    if method == \"rma\":\n",
    "        atr = tr.ewm(alpha=1.0/n, adjust=False).mean(); tag = \"rma\"\n",
    "    elif method == \"sma\":\n",
    "        atr = tr.rolling(window=n, min_periods=1).mean(); tag = \"sma\"\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'rma' or 'sma'\")\n",
    "\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    if include_tr:\n",
    "        out[f\"{out_prefix}_tr\"] = tr\n",
    "    out[f\"{out_prefix}_{n}_{tag}\"] = atr\n",
    "    if pct:\n",
    "        out[f\"{out_prefix}_{n}_{tag}_pct\"] = (atr / close)\n",
    "\n",
    "    return out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "# -----------------------------\n",
    "# Parkinson volatility\n",
    "# -----------------------------\n",
    "def parkinson_variance(high: pd.Series, low: pd.Series, window: int, min_periods: int | None = None) -> pd.Series:\n",
    "    ln_hl = np.log(high / low)\n",
    "    factor = 1.0 / (4.0 * np.log(2.0))\n",
    "    mp = window if min_periods is None else min_periods\n",
    "    rolling_sum_sq = ln_hl.pow(2).rolling(window=window, min_periods=mp).sum()\n",
    "    return factor * (rolling_sum_sq / window)\n",
    "\n",
    "def parkinson_volatility(high: pd.Series, low: pd.Series, window: int, min_periods: int | None = None) -> pd.Series:\n",
    "    return np.sqrt(parkinson_variance(high, low, window, min_periods=min_periods))\n",
    "\n",
    "# -----------------------------\n",
    "# Realised volatility (sqrt(sum r^2))\n",
    "# -----------------------------\n",
    "def realised_volatility(returns: pd.Series, window: int, min_periods: int | None = None) -> pd.Series:\n",
    "    mp = window if min_periods is None else min_periods\n",
    "    return (returns.pow(2).rolling(window=window, min_periods=mp).sum()).pow(0.5)\n",
    "\n",
    "# -----------------------------\n",
    "# Bollinger Bands\n",
    "# -----------------------------\n",
    "def calculate_bollinger_bands(price: pd.Series, windows: list[int] = [14, 30, 50]) -> pd.DataFrame:\n",
    "    features = {}\n",
    "    for w in windows:\n",
    "        sma = price.rolling(window=w).mean()\n",
    "        std = price.rolling(window=w).std()\n",
    "        upper = sma + 2*std\n",
    "        lower = sma - 2*std\n",
    "        signal = np.where(price > upper, 1, np.where(price < lower, -1, 0))\n",
    "        features[f'sma_{w}'] = sma\n",
    "        features[f'bb_width_{w}'] = (upper - lower)\n",
    "        features[f'bb_signal_{w}'] = signal\n",
    "    out = pd.DataFrame(features, index=price.index)\n",
    "    return out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "# -----------------------------\n",
    "# RSI\n",
    "# -----------------------------\n",
    "def compute_rsi(series: pd.Series, window: int, method: str = \"wilder\") -> pd.Series:\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "\n",
    "    if method == \"wilder\":\n",
    "        avg_gain = gain.ewm(alpha=1/window, adjust=False).mean()\n",
    "        avg_loss = loss.ewm(alpha=1/window, adjust=False).mean()\n",
    "    elif method == \"sma\":\n",
    "        avg_gain = gain.rolling(window, min_periods=1).mean()\n",
    "        avg_loss = loss.rolling(window, min_periods=1).mean()\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'wilder' or 'sma'\")\n",
    "\n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi.replace([np.inf, -np.inf], np.nan).fillna(50.0)\n",
    "\n",
    "def compute_multi_rsi(df: pd.DataFrame, price_col: str = \"close\", windows: list[int] = [7, 14, 21, 50]) -> pd.DataFrame:\n",
    "    close = df[price_col].astype(float)\n",
    "    out = {f\"rsi_{w}\": compute_rsi(close, window=w, method=\"wilder\") for w in windows}\n",
    "    return pd.DataFrame(out, index=df.index)\n",
    "\n",
    "# -----------------------------\n",
    "# Volatility-of-Volatility (VoV) ‚Äî fast & safe\n",
    "# -----------------------------\n",
    "def add_vov_features_fast(\n",
    "    df: pd.DataFrame,\n",
    "    vol_cols: list[str] | None = None,\n",
    "    windows: tuple[int, ...] = (60, 240),\n",
    "    log_space: bool = True,\n",
    "    prefix: str = \"vov\",\n",
    "    use_min_periods_full: bool = True,\n",
    "    eps: float = 1e-12,\n",
    ") -> pd.DataFrame:\n",
    "    if vol_cols is None:\n",
    "        vol_cols = [c for c in df.columns if any(k in c.lower() for k in (\"atr\", \"parkinson\", \"realised\", \"rv\", \"vol\"))]\n",
    "    minp = (lambda w: w) if use_min_periods_full else (lambda w: 1)\n",
    "\n",
    "    new = {}\n",
    "    for col in vol_cols:\n",
    "        base = df[col]\n",
    "        # If duplicate column names exist, df[col] is a DataFrame; take the first occurrence\n",
    "        if isinstance(base, pd.DataFrame):\n",
    "            base = base.iloc[:, 0]\n",
    "        base = pd.to_numeric(base, errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "        for w in windows:\n",
    "            vlin = base.rolling(w, min_periods=minp(w)).std()\n",
    "            new[f\"{prefix}_{col}_{w}\"] = vlin.to_numpy()\n",
    "\n",
    "            if log_space:\n",
    "                pos = base > 0\n",
    "                log_base = pd.Series(np.nan, index=base.index, dtype=\"float64\")\n",
    "                log_base[pos] = np.log(base[pos] + eps)\n",
    "                vlog = log_base.rolling(w, min_periods=minp(w)).std()\n",
    "                new[f\"{prefix}_{col}_log_{w}\"] = vlog.to_numpy()\n",
    "\n",
    "    block = pd.DataFrame(new, index=df.index)\n",
    "    block = block.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    block = downcast_floats(block, \"float32\")\n",
    "    return pd.concat([df, block], axis=1, copy=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Build the full volatility feature block\n",
    "# -----------------------------\n",
    "def build_volatility_features(df_filtered: pd.DataFrame) -> pd.DataFrame:\n",
    "    assert {'open','high','low','close'}.issubset(df_filtered.columns), \"df_filtered must have OHLC\"\n",
    "    canon = df_filtered.index\n",
    "    prices = df_filtered[['open','high','low','close']].astype(float)\n",
    "\n",
    "    # ATRs (compute *_tr once to avoid duplicate names)\n",
    "    df_atr_14_rma = add_atr(prices, n=14, method=\"rma\", pct=True, out_prefix=\"atr\", include_tr=True)\n",
    "    df_atr_14_sma = add_atr(prices, n=14, method=\"sma\", pct=True, out_prefix=\"atr_sma\", include_tr=False)\n",
    "    df_atr_24_rma = add_atr(prices, n=24, method=\"rma\", pct=True, out_prefix=\"atr\", include_tr=False)\n",
    "    df_atr_24_sma = add_atr(prices, n=24, method=\"sma\", pct=True, out_prefix=\"atr_sma\", include_tr=False)\n",
    "    df_atr_50_rma = add_atr(prices, n=50, method=\"rma\", pct=True, out_prefix=\"atr\", include_tr=False)\n",
    "    df_atr_50_sma = add_atr(prices, n=50, method=\"sma\", pct=True, out_prefix=\"atr_sma\", include_tr=False)\n",
    "\n",
    "    atr_frames = [ensure_reindex(x, canon) for x in\n",
    "                  [df_atr_14_rma, df_atr_14_sma, df_atr_24_rma, df_atr_24_sma, df_atr_50_rma, df_atr_50_sma]]\n",
    "    df_atr = pd.concat(atr_frames, axis=1, join=\"inner\", copy=False).copy()\n",
    "\n",
    "    # Parkinson volatility\n",
    "    H, L, C = prices['high'], prices['low'], prices['close']\n",
    "    par_wins = [5,10,15,30,60,120,240,380,1380]\n",
    "    df_par = pd.DataFrame(\n",
    "        {f\"parkinson_vol_{w}\": parkinson_volatility(H, L, window=w, min_periods=w) for w in par_wins},\n",
    "        index=canon\n",
    "    ).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # Realised volatility (log-returns)\n",
    "    returns = np.log(C).diff()\n",
    "    rv_wins = [5,10,15,30,60,120,240,380,1380]\n",
    "    df_rv = pd.DataFrame(\n",
    "        {f\"realised_vol_{w}\": realised_volatility(returns, window=w, min_periods=w) for w in rv_wins},\n",
    "        index=canon\n",
    "    ).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # RSI & Bollinger\n",
    "    df_rsi  = compute_multi_rsi(df_filtered, price_col=\"close\", windows=[7,14,21,50]).reindex(canon)\n",
    "    df_boll = calculate_bollinger_bands(C, windows=[14,30,50]).reindex(canon)\n",
    "\n",
    "    # Assemble core and de-duplicate any accidental name collisions\n",
    "    core = pd.concat([df_atr, df_par, df_rv, df_rsi, df_boll], axis=1, join=\"inner\", copy=False).copy()\n",
    "    core = core.loc[:, ~core.columns.duplicated()].copy()\n",
    "\n",
    "    # Add VoV over volatility-like columns\n",
    "    vol_cols = [c for c in core.columns if any(k in c.lower() for k in (\"atr\", \"parkinson\", \"realised\", \"rv\", \"vol\"))]\n",
    "    features = add_vov_features_fast(\n",
    "        core,\n",
    "        vol_cols=vol_cols,\n",
    "        windows=(60, 240),\n",
    "        log_space=True,\n",
    "        prefix=\"vov\",\n",
    "        use_min_periods_full=True,\n",
    "        eps=1e-12,\n",
    "    )\n",
    "\n",
    "    # Final clean + downcast\n",
    "    features = features.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    features = downcast_floats(features, \"float32\")\n",
    "\n",
    "    assert len(features) == len(df_filtered), \"Length mismatch after assembly\"\n",
    "    return features\n",
    "\n",
    "# -----------------------------\n",
    "# USAGE\n",
    "# -----------------------------\n",
    "df_volatility_features = build_volatility_features(df_filtered)\n",
    "print(\"df_volatility_features created with shape:\", df_volatility_features.shape)\n",
    "assert len(df_volatility_features) == len(df_filtered), \"Length mismatch!\"\n",
    "\n",
    "df_volatility_features.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea67066",
   "metadata": {},
   "source": [
    "### Momentum & Trend features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240ac6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_momentum_features created with shape: (1691325, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ema_12                     0\n",
       "ema_36                     0\n",
       "ema_72                     0\n",
       "ema_150                    0\n",
       "ema_300                    0\n",
       "ema_1000                   0\n",
       "macd_12_36                 0\n",
       "macd_12_36_signal          0\n",
       "macd_12_36_div             0\n",
       "macd_36_72                 0\n",
       "macd_36_72_signal          0\n",
       "macd_36_72_div             0\n",
       "macd_72_150                0\n",
       "macd_72_150_signal         0\n",
       "macd_72_150_div            0\n",
       "macd_150_300               0\n",
       "macd_150_300_signal        0\n",
       "macd_150_300_div           0\n",
       "macd_300_1000              0\n",
       "macd_300_1000_signal       0\n",
       "macd_300_1000_div          0\n",
       "ema_spread_12_36           0\n",
       "ema_spread_pct_12_36       0\n",
       "ema_spread_36_72           0\n",
       "ema_spread_pct_36_72       0\n",
       "ema_spread_72_150          0\n",
       "ema_spread_pct_72_150      0\n",
       "ema_spread_150_300         0\n",
       "ema_spread_pct_150_300     0\n",
       "ema_spread_300_1000        0\n",
       "ema_spread_pct_300_1000    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Momentum / Trend Features\n",
    "# ===============================\n",
    "\n",
    "# Exponential Moving Averages (EMA)\n",
    "# ------------------------------\n",
    "def ema(series: pd.Series, span: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute Exponential Moving Average (EMA) with standard finance conventions.\n",
    "    Uses adjust=False for true EMA behaviour.\n",
    "    \"\"\"\n",
    "    return series.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "close = df_filtered[\"close\"].astype(float)\n",
    "\n",
    "ema_12   = ema(close, 12)\n",
    "ema_36   = ema(close, 36)\n",
    "ema_72   = ema(close, 72)\n",
    "ema_150  = ema(close, 150)\n",
    "ema_300  = ema(close, 300)\n",
    "ema_1000 = ema(close, 1000)\n",
    "\n",
    "df_ema = pd.DataFrame(index=df_filtered.index)\n",
    "df_ema[\"ema_12\"]   = ema_12\n",
    "df_ema[\"ema_36\"]   = ema_36\n",
    "df_ema[\"ema_72\"]   = ema_72\n",
    "df_ema[\"ema_150\"]  = ema_150\n",
    "df_ema[\"ema_300\"]  = ema_300\n",
    "df_ema[\"ema_1000\"] = ema_1000\n",
    "\n",
    "assert len(df_ema) == df_length\n",
    "# MACD (Moving Average Convergence Divergence)\n",
    "# ------------------------------\n",
    "\n",
    "def macd(series: pd.Series, fast: int, slow: int, signal: int = 9, prefix: str = \"macd\"):\n",
    "    \"\"\"\n",
    "    Compute MACD line, signal line (EMA of MACD), and divergence (histogram).\n",
    "    Returns a DataFrame with columns: <prefix>, <prefix>_signal, <prefix>_div.\n",
    "    \"\"\"\n",
    "    ema_fast = ema(series, fast)\n",
    "    ema_slow = ema(series, slow)\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    macd_signal = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    macd_div = macd_line - macd_signal  # histogram / divergence\n",
    "\n",
    "    out = pd.DataFrame(index=series.index)\n",
    "    out[f\"{prefix}\"] = macd_line\n",
    "    out[f\"{prefix}_signal\"] = macd_signal\n",
    "    out[f\"{prefix}_div\"] = macd_div\n",
    "    return out\n",
    "\n",
    "close = df_filtered[\"close\"].astype(float)\n",
    "\n",
    "# --- Primary MACD using your windows: fast=12, slow=36, signal=9 ---\n",
    "df_macd_12_36 = macd(close, fast=12, slow=36, signal=9, prefix=\"macd_12_36\")\n",
    "\n",
    "# MACDs at multiple regimes using list:\n",
    "pairs = [(12,36), (36,72), (72,150), (150,300), (300,1000)]\n",
    "dfs = [df_macd_12_36]\n",
    "for f,s in pairs[1:]:\n",
    "    dfs.append(macd(close, fast=f, slow=s, signal=9, prefix=f\"macd_{f}_{s}\"))\n",
    "\n",
    "# Concatenate all MACD variants if you generated multiple\n",
    "df_macd = pd.concat(dfs, axis=1)\n",
    "\n",
    "# Optional: if using these as ML features, lag by 1 to avoid leakage:\n",
    "df_macd = df_macd.shift(1).fillna(0.0)\n",
    "assert len(df_macd) == df_length\n",
    "\n",
    "# EMA Spreads (absolute and percent)\n",
    "# ----------------------------------\n",
    "def ema_spread_abs(fast: pd.Series, slow: pd.Series) -> pd.Series:\n",
    "    return (fast - slow)\n",
    "\n",
    "def ema_spread_pct(fast: pd.Series, slow: pd.Series) -> pd.Series:\n",
    "    return (fast / slow - 1.0).replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "pairs = [(12,36), (36,72), (72,150), (150,300), (300,1000)]\n",
    "\n",
    "df_ema_spread = pd.DataFrame(index=df_filtered.index)\n",
    "\n",
    "for f, s in pairs:\n",
    "    fast = df_ema[f\"ema_{f}\"]\n",
    "    slow = df_ema[f\"ema_{s}\"]\n",
    "    df_ema_spread[f\"ema_spread_{f}_{s}\"]      = ema_spread_abs(fast, slow)\n",
    "    df_ema_spread[f\"ema_spread_pct_{f}_{s}\"]  = ema_spread_pct(fast, slow)\n",
    "\n",
    "# Optional: align for ML like you did for MACD (avoid leakage)\n",
    "df_ema_spread = df_ema_spread.shift(1).fillna(0.0)\n",
    "\n",
    "assert len(df_ema_spread) == df_length\n",
    "\n",
    "df_momentum_features = pd.concat([df_ema, df_macd, df_ema_spread], axis=1)\n",
    "assert len(df_momentum_features) == df_length\n",
    "print(\"df_momentum_features created with shape:\", df_momentum_features.shape)\n",
    "df_momentum_features.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5751724",
   "metadata": {},
   "source": [
    "### Volume & Order Flow Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d99b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_vol_z created with shape: (1691325, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vol_z_5          0\n",
       "vol_z_20         0\n",
       "vol_z_60         0\n",
       "vol_z_120        0\n",
       "vol_z_240        0\n",
       "vol_z_390        0\n",
       "vol_z_log_5      0\n",
       "vol_z_log_20     0\n",
       "vol_z_log_60     0\n",
       "vol_z_log_120    0\n",
       "vol_z_log_240    0\n",
       "vol_z_log_390    0\n",
       "obv              0\n",
       "obv_norm         1\n",
       "obv_change       0\n",
       "obv_z_200        0\n",
       "cmf_20           0\n",
       "cmf_10           0\n",
       "cmf_60           0\n",
       "cmf_120          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Volume and Order Flow Features\n",
    "# ===============================\n",
    "\n",
    "def add_volume_zscores(\n",
    "    volume: pd.Series,\n",
    "    windows: list[int],\n",
    "    log_space: bool = False,\n",
    "    deseasonalize_intraday: bool = False,\n",
    "    clip: float | None = None,\n",
    "    prefix: str = \"vol_z\",\n",
    "    eps: float = 1e-9,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute volume z-scores over multiple rolling windows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume : pd.Series\n",
    "        Per-bar volume (e.g., 1-min). Index should be a DateTimeIndex (UTC is fine).\n",
    "    windows : list[int]\n",
    "        Rolling window sizes (in bars), e.g., [5, 20, 60, 120, 240, 390].\n",
    "    log_space : bool, default False\n",
    "        If True, compute z-scores in log(volume) space (more stable).\n",
    "    deseasonalize_intraday : bool, default False\n",
    "        If True, remove intraday seasonality by minute-of-day before z-scoring.\n",
    "        (Linear in level space; in log space, removes the average log-volume pattern.)\n",
    "    clip : float | None, default None\n",
    "        If set, clip z-scores to [-clip, clip].\n",
    "    prefix : str, default \"vol_z\"\n",
    "        Prefix for output column names.\n",
    "    eps : float, default 1e-9\n",
    "        Small constant to avoid log(0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns like f\"{prefix}_{w}\" for each window w.\n",
    "    \"\"\"\n",
    "    vol = volume.astype(float).copy()\n",
    "    assert isinstance(vol.index, pd.DatetimeIndex), \"volume index must be a DateTimeIndex\"\n",
    "\n",
    "    # Build base series for z-scoring (with optional de-seasonalization)\n",
    "    if log_space:\n",
    "        logv = np.log(vol.clip(lower=eps))\n",
    "        if deseasonalize_intraday:\n",
    "            # remove minute-of-day mean in log space: logv - E[logv | minute_of_day]\n",
    "            mod = logv.groupby(logv.index.time).transform(\"mean\")\n",
    "            base = (logv - mod)\n",
    "        else:\n",
    "            base = logv\n",
    "    else:\n",
    "        if deseasonalize_intraday:\n",
    "            # remove minute-of-day mean in level space: vol - E[vol | minute_of_day]\n",
    "            mod = vol.groupby(vol.index.time).transform(\"mean\")\n",
    "            base = (vol - mod)\n",
    "        else:\n",
    "            base = vol\n",
    "\n",
    "    out = pd.DataFrame(index=vol.index)\n",
    "    for w in windows:\n",
    "        m = base.rolling(w, min_periods=1).mean()\n",
    "        s = base.rolling(w, min_periods=1).std()\n",
    "        z = (base - m) / s.replace(0, np.nan)\n",
    "\n",
    "        if clip is not None:\n",
    "            z = z.clip(-clip, clip)\n",
    "\n",
    "        z = z.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "        name = f\"{prefix}_{'log_' if log_space else ''}{w}\"\n",
    "        out[name] = z\n",
    "\n",
    "    return out\n",
    "\n",
    "vol = df_filtered[\"volume\"]  # your per-bar volume series\n",
    "windows = [5, 20, 60, 120, 240, 390]\n",
    "\n",
    "# Plain volume z-scores\n",
    "vz = add_volume_zscores(vol, windows=windows, log_space=False, deseasonalize_intraday=False)\n",
    "\n",
    "# Log-volume, de-seasonalized (recommended)\n",
    "vz_log = add_volume_zscores(vol, windows=windows, log_space=True, deseasonalize_intraday=True, clip=8)\n",
    "\n",
    "# On-Balance Volume (OBV)\n",
    "# ------------------------------\n",
    "def obv(close: pd.Series, volume: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute On-Balance Volume (OBV).\n",
    "\n",
    "    OBV_t = OBV_{t-1} + sign(close_t - close_{t-1}) * volume_t\n",
    "\n",
    "    +volume added if price up, -volume if price down, unchanged if flat.\n",
    "    \"\"\"\n",
    "    close = close.astype(float)\n",
    "    volume = volume.astype(float).fillna(0.0)\n",
    "\n",
    "    # Price direction: +1 up, -1 down, 0 flat\n",
    "    direction = (close.diff().fillna(0.0)).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "\n",
    "    # OBV cumulative\n",
    "    obv_series = (direction * volume).cumsum()\n",
    "\n",
    "    return obv_series\n",
    "close = df_filtered[\"close\"]\n",
    "vol = df_filtered[\"volume\"]\n",
    "\n",
    "df_obv = pd.DataFrame(index=df_filtered.index)\n",
    "df_obv[\"obv\"] = obv(close, vol)\n",
    "\n",
    "# enhancements for ML\n",
    "df_obv[\"obv_norm\"] = df_obv[\"obv\"] / df_obv[\"obv\"].abs().rolling(2000, min_periods=1).max()\n",
    "df_obv[\"obv_change\"] = df_obv[\"obv\"].diff().fillna(0.0)\n",
    "def zscore(series, window: int):\n",
    "    r = series.rolling(window, min_periods=1)\n",
    "    return (series - r.mean()) / r.std().replace(0, np.nan)\n",
    "\n",
    "df_obv[\"obv_z_200\"] = zscore(df_obv[\"obv\"], 200).fillna(0.0)\n",
    "\n",
    "# Chaikin Money Flow (CMF)\n",
    "# ------------------------------\n",
    "\n",
    "def chaikin_money_flow(\n",
    "    high: pd.Series,\n",
    "    low: pd.Series,\n",
    "    close: pd.Series,\n",
    "    volume: pd.Series,\n",
    "    window: int = 20,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Chaikin Money Flow (CMF): volume-weighted buying/selling pressure over a window.\n",
    "    \n",
    "    CMF_t = sum_{i=t-w+1..t} [ MFM_i * Volume_i ] / sum_{i=t-w+1..t} [ Volume_i ]\n",
    "    where MFM_i (Money Flow Multiplier) = ((Close - Low) - (High - Close)) / (High - Low)\n",
    "    \n",
    "    - Returns a pandas Series aligned to the input index.\n",
    "    - Safe for bars with High == Low (treats MFM as 0 for that bar).\n",
    "    \"\"\"\n",
    "    high   = high.astype(float)\n",
    "    low    = low.astype(float)\n",
    "    close  = close.astype(float)\n",
    "    volume = volume.astype(float).fillna(0.0)\n",
    "\n",
    "    # Money Flow Multiplier (handle high==low as 0)\n",
    "    denom = (high - low)\n",
    "    mfm = ((close - low) - (high - close))\n",
    "    mfm = mfm.where(denom != 0, 0.0) / denom.replace(0, np.nan)\n",
    "\n",
    "    # Money Flow Volume\n",
    "    mfv = mfm * volume\n",
    "\n",
    "    # Rolling sums\n",
    "    sum_mfv = mfv.rolling(window, min_periods=1).sum()\n",
    "    sum_vol = volume.rolling(window, min_periods=1).sum()\n",
    "\n",
    "    # CMF\n",
    "    cmf = (sum_mfv / sum_vol.replace(0, np.nan)).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    cmf.name = f\"cmf_{window}\"\n",
    "    return cmf\n",
    "\n",
    "cmf_20 = chaikin_money_flow(\n",
    "    df_filtered[\"high\"],\n",
    "    df_filtered[\"low\"],\n",
    "    df_filtered[\"close\"],\n",
    "    df_filtered[\"volume\"],\n",
    "    window=20\n",
    ")\n",
    "\n",
    "# Add to a DF of features\n",
    "df_volume_flow = pd.DataFrame(index=df_filtered.index)\n",
    "df_volume_flow[\"cmf_20\"] = cmf_20\n",
    "\n",
    "# Optional: multiple windows\n",
    "for w in (10, 20, 60, 120):\n",
    "    df_volume_flow[f\"cmf_{w}\"] = chaikin_money_flow(\n",
    "        df_filtered[\"high\"], df_filtered[\"low\"], df_filtered[\"close\"], df_filtered[\"volume\"], window=w\n",
    "    )\n",
    "\n",
    "# Optional: lag for ML to avoid leakage\n",
    "df_volume_flow = df_volume_flow.shift(1).fillna(0.0)\n",
    "\n",
    "\n",
    "# Append to your feature DF\n",
    "df_vol_z = pd.concat([vz, vz_log, df_obv, df_volume_flow], axis=1)\n",
    "assert len(df_vol_z) == df_length\n",
    "print(\"df_vol_z created with shape:\", df_vol_z.shape)\n",
    "df_vol_z.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96281348",
   "metadata": {},
   "source": [
    "# Volume & Order-Flow Features (exact outputs)\n",
    "\n",
    "Below are the **column names** created by your code and what each represents.  \n",
    "Final DF: `df_vol_z = concat([vz, vz_log, df_obv, df_volume_flow])`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Volume z-scores (level space)\n",
    "- **vol_z_5, vol_z_20, vol_z_60, vol_z_120, vol_z_240, vol_z_390**  \n",
    "  Z-score of per-bar **volume** over rolling window *w* (level space, no intraday de-seasonalization).\n",
    "\n",
    "## 2) Volume z-scores (log space, de-seasonalized)\n",
    "- **vol_z_log_5, vol_z_log_20, vol_z_log_60, vol_z_log_120, vol_z_log_240, vol_z_log_390**  \n",
    "  Z-score of **log(volume)** after removing **minute-of-day mean** (stabilized, clipped to ¬±8).\n",
    "\n",
    "## 3) On-Balance Volume (OBV) family\n",
    "- **obv** ‚Äî Cumulative signed volume (adds volume when price up, subtracts when down).  \n",
    "- **obv_norm** ‚Äî `obv` normalized by rolling **max(|obv|)** over 2000 bars (range ‚âà -1‚Ä¶1).  \n",
    "- **obv_change** ‚Äî First difference of `obv` (per-bar OBV flow).  \n",
    "- **obv_z_200** ‚Äî Z-score of `obv` over a 200-bar window.\n",
    "\n",
    "## 4) Chaikin Money Flow (CMF) ‚Äî **shifted by 1 bar** (leakage-safe)\n",
    "- **cmf_10, cmf_20, cmf_60, cmf_120**  \n",
    "  Volume-weighted buying/selling pressure over window *w* (uses MFM √ó Volume / sum Volume).\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- `vz` and `vz_log` are **not shifted**; `df_volume_flow` (CMF block) is **shifted by 1 bar**.  \n",
    "- Windows used: `[5, 20, 60, 120, 240, 390]` for z-scores; `(10, 20, 60, 120)` for CMF.  \n",
    "- All NaNs/¬±‚àû from divisions are cleaned as per code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8f707",
   "metadata": {},
   "source": [
    "### VWAP & Fair-value distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75891258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_vwap_features created with shape: (1691325, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vwap_w5            0\n",
       "dist_vwap_w5       0\n",
       "dist_vwap_z_w5     0\n",
       "vwap_w10           0\n",
       "dist_vwap_w10      0\n",
       "dist_vwap_z_w10    0\n",
       "vwap_w15           0\n",
       "dist_vwap_w15      0\n",
       "dist_vwap_z_w15    0\n",
       "vwap_w30           0\n",
       "dist_vwap_w30      0\n",
       "dist_vwap_z_w30    0\n",
       "vwap_w60           0\n",
       "dist_vwap_w60      0\n",
       "dist_vwap_z_w60    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VWAP and fair value distance\n",
    "# -------------------------------\n",
    "\n",
    "# -------- rolling VWAP (typical-price weighted) --------\n",
    "def rolling_vwap(\n",
    "    df: pd.DataFrame,\n",
    "    window: int,\n",
    "    use_typical: bool = True,\n",
    "    gap_safe: bool = True,\n",
    "    gap_timedelta: str = \"2min\",\n",
    "    hi_col: str = \"high\",\n",
    "    lo_col: str = \"low\",\n",
    "    cl_col: str = \"close\",\n",
    "    vol_col: str = \"volume\",\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    VWAP_t(window) = sum_{t-window+1..t} (TP * Vol) / sum(Vol),\n",
    "    TP = (H+L+C)/3 if use_typical else Close.\n",
    "    Gap-safe: sets vol to 0 on bars that open after a big time gap (> gap_timedelta),\n",
    "              so gaps don't smear VWAP across sessions.\n",
    "    \"\"\"\n",
    "    assert {hi_col, lo_col, cl_col, vol_col}.issubset(df.columns)\n",
    "    h = df[hi_col].astype(float)\n",
    "    l = df[lo_col].astype(float)\n",
    "    c = df[cl_col].astype(float)\n",
    "    v = df[vol_col].astype(float).replace(np.nan, 0.0)\n",
    "\n",
    "    if gap_safe:\n",
    "        gap_mask = df.index.to_series().diff() > pd.Timedelta(gap_timedelta)\n",
    "        # zero volume on the first bar after a gap (prevents cross-session blending)\n",
    "        v = v.where(~gap_mask, 0.0)\n",
    "\n",
    "    tp = ((h + l + c) / 3.0) if use_typical else c\n",
    "    tpv = (tp * v)\n",
    "\n",
    "    num = tpv.rolling(window, min_periods=1).sum()\n",
    "    den = v.rolling(window, min_periods=1).sum()\n",
    "    vw = (num / den.replace(0, np.nan))\n",
    "\n",
    "    return vw.replace([np.inf, -np.inf], np.nan).ffill().replace(np.nan, 0.0)\n",
    "\n",
    "\n",
    "# -------- lightweight ATR (only if you don't already have one) --------\n",
    "def _atr_wilder(\n",
    "    df: pd.DataFrame,\n",
    "    n: int = 14,\n",
    "    hi_col: str = \"high\",\n",
    "    lo_col: str = \"low\",\n",
    "    cl_col: str = \"close\",\n",
    "    gap_safe: bool = True,\n",
    "    gap_timedelta: str = \"2min\",\n",
    ") -> pd.Series:\n",
    "    h = df[hi_col].astype(float)\n",
    "    l = df[lo_col].astype(float)\n",
    "    c = df[cl_col].astype(float)\n",
    "    prev_c = c.shift(1)\n",
    "    if gap_safe:\n",
    "        gap_mask = df.index.to_series().diff() > pd.Timedelta(gap_timedelta)\n",
    "        prev_c = prev_c.where(~gap_mask, c)\n",
    "    tr = pd.concat([(h - l).abs(), (h - prev_c).abs(), (l - prev_c).abs()], axis=1).max(axis=1)\n",
    "    atr = tr.ewm(alpha=1.0/n, adjust=False).mean()\n",
    "    return atr.replace([np.inf, -np.inf], 0.0).replace(np.nan, 0.0)\n",
    "\n",
    "\n",
    "# -------- feature builder: VWAP, dist, dist/ATR --------\n",
    "def build_vwap_features(\n",
    "    df: pd.DataFrame,\n",
    "    windows: list[int] = (5, 10, 15, 30, 60),\n",
    "    atr_col: str | None = \"atr_14_rma\",   # pass your existing ATR column name; if None, auto-compute\n",
    "    atr_n: int = 14,\n",
    "    shift_by_one: bool = True,            # good practice to avoid leakage\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns columns per window:\n",
    "      - vwap_w{w}\n",
    "      - dist_vwap_w{w}      = close - vwap\n",
    "      - dist_vwap_z_w{w}    = (close - vwap) / ATR\n",
    "    \"\"\"\n",
    "    assert {\"high\",\"low\",\"close\",\"volume\"}.issubset(df.columns)\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    close = df[\"close\"].astype(float)\n",
    "\n",
    "    # ATR source\n",
    "    if atr_col is not None and atr_col in df.columns:\n",
    "        atr = df[atr_col].astype(float)\n",
    "    else:\n",
    "        atr = _atr_wilder(df, n=atr_n)\n",
    "\n",
    "    # build per-window features\n",
    "    for w in windows:\n",
    "        vwap_w = rolling_vwap(df, window=w, use_typical=True)\n",
    "        dist_raw = (close - vwap_w)\n",
    "        dist_z = dist_raw / atr.replace(0, np.nan)\n",
    "\n",
    "        out[f\"vwap_w{w}\"] = vwap_w\n",
    "        out[f\"dist_vwap_w{w}\"] = dist_raw\n",
    "        out[f\"dist_vwap_z_w{w}\"] = dist_z.replace([np.inf, -np.inf], 0.0).replace(np.nan, 0.0)\n",
    "\n",
    "    # hygiene + optional lag\n",
    "    out = out.replace([np.inf, -np.inf], 0.0).replace(np.nan, 0.0)\n",
    "    if shift_by_one:\n",
    "        out = out.shift(1).replace(np.nan, 0.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "# If you already created ATR columns with your add_atr function, pass that column name:\n",
    "# e.g., df_filtered[\"atr_14_rma\"]\n",
    "df_vwap_features = build_vwap_features(\n",
    "    df_filtered,\n",
    "    windows=[5, 10, 15, 30, 60],\n",
    "    atr_col=\"atr_14_rma\",   # or None to auto-compute\n",
    "    atr_n=14,\n",
    "    shift_by_one=True\n",
    ")\n",
    "\n",
    "assert len(df_vwap_features) == df_length\n",
    "print(\"df_vwap_features created with shape:\", df_vwap_features.shape)\n",
    "df_vwap_features.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c562fcf",
   "metadata": {},
   "source": [
    "### Microstructure Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0150f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_microstructure created with shape: (1691325, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "clv_01              0\n",
       "gap_pct             0\n",
       "dir_persist_w20     0\n",
       "dir_persist_w60     0\n",
       "dir_persist_w120    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Microstructure proxies \n",
    "# ===============================\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Close Location Value [0,1]\n",
    "# ------------------------------\n",
    "def clv_01(\n",
    "    df: pd.DataFrame,\n",
    "    hi_col: str = \"high\",\n",
    "    lo_col: str = \"low\",\n",
    "    cl_col: str = \"close\",\n",
    "    neutral: float = 0.5,         # value to use when (high==low)\n",
    "    clip: bool = True,            # clip to [0,1] for safety\n",
    "    shift_by_one: bool = True     # lag to avoid leakage if using as ML feature\n",
    ") -> pd.Series:\n",
    "    h = df[hi_col].astype(float)\n",
    "    l = df[lo_col].astype(float)\n",
    "    c = df[cl_col].astype(float)\n",
    "    rng = (h - l)\n",
    "\n",
    "    clv = (c - l) / rng.replace(0, np.nan)\n",
    "    # fallback when range==0\n",
    "    clv = clv.fillna(neutral)\n",
    "\n",
    "    if clip:\n",
    "        clv = clv.clip(lower=0.0, upper=1.0)\n",
    "\n",
    "    clv = clv.replace([np.inf, -np.inf], 0.0).fillna(neutral)\n",
    "    if shift_by_one:\n",
    "        clv = clv.shift(1).fillna(neutral)\n",
    "    return clv\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Gap percent between prev close and open\n",
    "#    gap_pct = abs(O - C_{t-1}) / abs(C_{t-1})\n",
    "#    If only_on_gaps=True, set to 0 unless a time gap > gap_timedelta occurs.\n",
    "# ------------------------------\n",
    "def gap_pct(\n",
    "    df: pd.DataFrame,\n",
    "    op_col: str = \"open\",\n",
    "    cl_col: str = \"close\",\n",
    "    only_on_gaps: bool = False,\n",
    "    gap_timedelta: str = \"2min\",\n",
    "    shift_by_one: bool = True\n",
    ") -> pd.Series:\n",
    "    o = df[op_col].astype(float)\n",
    "    c = df[cl_col].astype(float)\n",
    "    prev_c = c.shift(1)\n",
    "\n",
    "    g = (o - prev_c).abs() / prev_c.abs().replace(0, np.nan)\n",
    "    g = g.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    if only_on_gaps:\n",
    "        gap_mask = df.index.to_series().diff() > pd.Timedelta(gap_timedelta)\n",
    "        g = g.where(gap_mask, 0.0)\n",
    "\n",
    "    if shift_by_one:\n",
    "        g = g.shift(1).fillna(0.0)\n",
    "    return g\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Directional persistence\n",
    "#    Fraction of positive returns in rolling windows.\n",
    "#    You can pass a precomputed returns series; otherwise uses close-to-close pct_change().\n",
    "# ------------------------------\n",
    "def dir_persist(\n",
    "    df: pd.DataFrame,\n",
    "    windows: list[int] = (20, 60, 120),\n",
    "    returns: pd.Series | None = None,\n",
    "    cl_col: str = \"close\",\n",
    "    gap_safe: bool = True,\n",
    "    gap_timedelta: str = \"2min\",\n",
    "    shift_by_one: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    if returns is None:\n",
    "        c = df[cl_col].astype(float)\n",
    "        r = c.pct_change()\n",
    "        if gap_safe:\n",
    "            # zero out returns across big time gaps to avoid cross-session jumps\n",
    "            is_gap = df.index.to_series().diff() > pd.Timedelta(gap_timedelta)\n",
    "            r = r.where(~is_gap, 0.0)\n",
    "        r = r.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    else:\n",
    "        r = returns.astype(float).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "    pos = (r > 0).astype(float)\n",
    "\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    for w in windows:\n",
    "        out[f\"dir_persist_w{w}\"] = pos.rolling(window=w, min_periods=1).mean()\n",
    "\n",
    "    out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    if shift_by_one:\n",
    "        out = out.shift(1).fillna(0.0)\n",
    "    return out\n",
    "\n",
    "df_microstructure = pd.DataFrame(index=df_filtered.index)\n",
    "\n",
    "df_microstructure[\"clv_01\"] = clv_01(df_filtered)\n",
    "df_microstructure[\"gap_pct\"] = gap_pct(df_filtered, only_on_gaps=True, gap_timedelta=\"10min\")\n",
    "\n",
    "dirp = dir_persist(df_filtered, windows=[20, 60, 120], gap_timedelta=\"2min\")\n",
    "df_microstructure = pd.concat([df_microstructure, dirp], axis=1)\n",
    "\n",
    "# Sanity\n",
    "assert len(df_microstructure) == len(df_filtered)\n",
    "print(\"df_microstructure created with shape:\", df_microstructure.shape)\n",
    "\n",
    "df_microstructure.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b9df3",
   "metadata": {},
   "source": [
    "### Volume Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8794bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vp_unusual_inc_count_w5      0\n",
       "vp_unusual_inc_maxz_w5       0\n",
       "vp_unusual_inc_share_w5      0\n",
       "vp_unusual_inc_near_px_w5    0\n",
       "vp_unusual_dec_count_w5      0\n",
       "                            ..\n",
       "vp_vpoc_dist_atr_70_w60      0\n",
       "vp_above_vah_70_w60          0\n",
       "vp_below_val_70_w60          0\n",
       "vp_entropy_w60               0\n",
       "vp_topk_share_w60            0\n",
       "Length: 170, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Volume Profile Features\n",
    "# ===============================\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def _bin_edges(\n",
    "    prices: np.ndarray,\n",
    "    nBins: int,\n",
    "    price_min=None,\n",
    "    price_max=None,\n",
    "    tick: float | None = 0.25,   # use 0.25 for NQ\n",
    "):\n",
    "    \"\"\"\n",
    "    Build bin edges. Handles flat/NaN windows by expanding a tiny range\n",
    "    (or >= 1 tick) around the price so pmax>pmin always holds.\n",
    "    \"\"\"\n",
    "    prices = np.asarray(prices, dtype=float)\n",
    "    prices = prices[np.isfinite(prices)]\n",
    "    if prices.size == 0:\n",
    "        raise ValueError(\"No finite prices in window.\")\n",
    "\n",
    "    pmin = float(np.min(prices)) if price_min is None else float(price_min)\n",
    "    pmax = float(np.max(prices)) if price_max is None else float(price_max)\n",
    "\n",
    "    if not np.isfinite(pmin) or not np.isfinite(pmax):\n",
    "        raise ValueError(\"Invalid price range for volume profile (NaNs).\")\n",
    "\n",
    "    if pmax <= pmin:\n",
    "        # Flat window ‚Üí create a minimal band around the price\n",
    "        p = pmin\n",
    "        width = (tick if (tick and tick > 0) else max(abs(p) * 1e-6, 1e-6))\n",
    "        pmin = p - 0.5 * width\n",
    "        pmax = p + 0.5 * width\n",
    "\n",
    "    eps = (pmax - pmin) * 1e-12\n",
    "    return np.linspace(pmin, pmax + eps, nBins + 1)\n",
    "\n",
    "\n",
    "def _build_vp_df(edges: np.ndarray, agg_vol: np.ndarray) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"minPrice\": edges[:-1],\n",
    "        \"maxPrice\": edges[1:],\n",
    "        \"aggregateVolume\": agg_vol.astype(float)\n",
    "    })\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Price/Volume version\n",
    "# ---------------------------\n",
    "def getVP(df: pd.DataFrame, nBins: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df must contain: ['price', 'volume'].\n",
    "    Returns DataFrame with ['minPrice','maxPrice','aggregateVolume'] for each bin.\n",
    "    \"\"\"\n",
    "    assert {\"price\", \"volume\"}.issubset(df.columns), \"getVP expects columns: price, volume\"\n",
    "    price = df[\"price\"].astype(float).to_numpy()\n",
    "    vol   = df[\"volume\"].astype(float).fillna(0.0).to_numpy()\n",
    "\n",
    "    edges = _bin_edges(price, nBins)\n",
    "    # weighted histogram ‚Üí sum of volumes per price bin\n",
    "    agg_vol, _ = np.histogram(price, bins=edges, weights=vol)\n",
    "    return _build_vp_df(edges, agg_vol)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) OHLC/Volume version\n",
    "# ---------------------------\n",
    "def getVPWithOHLC(df: pd.DataFrame, nBins: int = 20, tick: float = 0.25) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df must contain: ['open','high','low','close','volume'].\n",
    "    Distributes each bar‚Äôs volume across bins proportional to overlap of [low, high].\n",
    "    \"\"\"\n",
    "    need = {\"open\",\"high\",\"low\",\"close\",\"volume\"}\n",
    "    assert need.issubset(df.columns), f\"getVPWithOHLC expects columns: {need}\"\n",
    "\n",
    "    low   = df[\"low\"].astype(float).to_numpy()\n",
    "    high  = df[\"high\"].astype(float).to_numpy()\n",
    "    close = df[\"close\"].astype(float).to_numpy()\n",
    "    vol   = df[\"volume\"].astype(float).fillna(0.0).to_numpy()\n",
    "\n",
    "    prices_for_range = np.concatenate([low, high, close])\n",
    "    finite = np.isfinite(prices_for_range)\n",
    "    if not finite.any():\n",
    "        # Return empty profile; caller should treat as \"no features\"\n",
    "        return pd.DataFrame(columns=[\"minPrice\",\"maxPrice\",\"aggregateVolume\"])\n",
    "\n",
    "    edges = _bin_edges(prices_for_range[finite], nBins, tick=tick)\n",
    "    agg_vol = np.zeros(nBins, dtype=float)\n",
    "\n",
    "    for lo, hi, cl, v in zip(low, high, close, vol):\n",
    "        if v <= 0 or not np.isfinite(v) or not (np.isfinite(lo) and np.isfinite(hi) and np.isfinite(cl)):\n",
    "            continue\n",
    "        if hi <= lo:\n",
    "            # Degenerate bar ‚Üí assign all vol to bin containing close\n",
    "            i = np.searchsorted(edges, cl, side=\"right\") - 1\n",
    "            if 0 <= i < nBins:\n",
    "                agg_vol[i] += v\n",
    "            continue\n",
    "        total_range = hi - lo\n",
    "        i_start = max(0, np.searchsorted(edges, lo, side=\"right\") - 1)\n",
    "        i_end   = min(nBins - 1, np.searchsorted(edges, hi, side=\"left\"))\n",
    "        for i in range(i_start, i_end + 1):\n",
    "            b_lo, b_hi = edges[i], edges[i+1]\n",
    "            overlap = max(0.0, min(hi, b_hi) - max(lo, b_lo))\n",
    "            if overlap > 0:\n",
    "                agg_vol[i] += v * (overlap / total_range)\n",
    "\n",
    "    return pd.DataFrame({\"minPrice\": edges[:-1], \"maxPrice\": edges[1:], \"aggregateVolume\": agg_vol})\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Top-K bins by volume\n",
    "# ---------------------------\n",
    "def getKMaxBars(volprofile_result: pd.DataFrame, k: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns top-k rows by aggregateVolume (descending).\n",
    "    \"\"\"\n",
    "    assert {\"minPrice\",\"maxPrice\",\"aggregateVolume\"}.issubset(volprofile_result.columns)\n",
    "    k = max(1, int(k))\n",
    "    return volprofile_result.sort_values(\"aggregateVolume\", ascending=False).head(k).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Unusual Increasing/Decreasing bars\n",
    "# ---------------------------\n",
    "def getUnusualIncreasingBars(vp: pd.DataFrame, isUpward: bool, z_thresh: float = 2.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies price bins where volume is 'unusually' increasing (isUpward=True)\n",
    "    or decreasing (isUpward=False) versus neighboring bins.\n",
    "\n",
    "    Method:\n",
    "      - Sort by bin mid-price ascending\n",
    "      - Compute first difference of aggregateVolume across bins\n",
    "      - Z-score the differences; keep bins where sign matches direction and |z| >= z_thresh\n",
    "    \"\"\"\n",
    "    assert {\"minPrice\",\"maxPrice\",\"aggregateVolume\"}.issubset(vp.columns)\n",
    "    df = vp.copy()\n",
    "    df[\"mid\"] = (df[\"minPrice\"] + df[\"maxPrice\"]) * 0.5\n",
    "    df = df.sort_values(\"mid\").reset_index(drop=True)\n",
    "\n",
    "    diff = df[\"aggregateVolume\"].diff()\n",
    "    mu   = diff.rolling(10, min_periods=1).mean()\n",
    "    sd   = diff.rolling(10, min_periods=1).std()\n",
    "    z    = (diff - mu) / sd.replace(0, np.nan)\n",
    "    z    = z.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    if isUpward:\n",
    "        mask = (diff > 0) & (z >= z_thresh)\n",
    "    else:\n",
    "        mask = (diff < 0) & (z <= -z_thresh)\n",
    "\n",
    "    out = df.loc[mask, [\"minPrice\",\"maxPrice\",\"aggregateVolume\"]].copy()\n",
    "    out[\"zscore_change\"] = z[mask].values\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Plot (simple horizontal volume profile)\n",
    "# ---------------------------\n",
    "def plot(vp: pd.DataFrame, price: pd.Series | np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Simple horizontal bar plot of the profile.\n",
    "    `price` is used to show current price line.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    df = vp.copy()\n",
    "    df[\"mid\"] = (df[\"minPrice\"] + df[\"maxPrice\"]) * 0.5\n",
    "    y = df[\"mid\"].to_numpy()\n",
    "    x = df[\"aggregateVolume\"].to_numpy()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(y, x, height=(df[\"maxPrice\"] - df[\"minPrice\"]).to_numpy(), align=\"center\")\n",
    "    if price is not None and len(price) > 0:\n",
    "        last = float(price.iloc[-1] if isinstance(price, pd.Series) else price[-1])\n",
    "        ax.axhline(last, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Aggregate Volume\")\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.set_title(\"Volume Profile\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------- helpers (coverage, entropy, concentration) ----------\n",
    "def _value_area(vp: pd.DataFrame, coverage: float) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Given VP df with ['minPrice','maxPrice','aggregateVolume'],\n",
    "    return (VPOC_mid, VA_low, VA_high) for a target coverage (e.g., 0.40 or 0.70).\n",
    "    \"\"\"\n",
    "    vp = vp.copy()\n",
    "    vp[\"mid\"] = (vp[\"minPrice\"] + vp[\"maxPrice\"]) * 0.5\n",
    "    total = float(vp[\"aggregateVolume\"].sum())\n",
    "    if total <= 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # VPOC mid\n",
    "    vpoc_mid = float(vp.loc[vp[\"aggregateVolume\"].idxmax(), \"mid\"])\n",
    "\n",
    "    # Greedy fill of largest bins to reach coverage\n",
    "    vp_sorted = vp.sort_values(\"aggregateVolume\", ascending=False)\n",
    "    vp_sorted[\"cum\"] = (vp_sorted[\"aggregateVolume\"].cumsum() / total)\n",
    "    keep = vp_sorted.loc[vp_sorted[\"cum\"] <= coverage]\n",
    "    if keep.shape[0] < vp_sorted.shape[0]:\n",
    "        keep = pd.concat([keep, vp_sorted.iloc[[keep.shape[0]]]])\n",
    "\n",
    "    if keep.empty:\n",
    "        return vpoc_mid, np.nan, np.nan\n",
    "\n",
    "    va_low = float(keep[\"minPrice\"].min())\n",
    "    va_high = float(keep[\"maxPrice\"].max())\n",
    "    return vpoc_mid, va_low, va_high\n",
    "\n",
    "def _profile_entropy(vp: pd.DataFrame) -> float:\n",
    "    vol = vp[\"aggregateVolume\"].to_numpy(dtype=float)\n",
    "    s = vol.sum()\n",
    "    if s <= 0: \n",
    "        return 0.0\n",
    "    p = vol / s\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "def _topk_share(vp: pd.DataFrame, k: int = 3) -> float:\n",
    "    vol = vp[\"aggregateVolume\"].sort_values(ascending=False).to_numpy(dtype=float)\n",
    "    s = vol.sum()\n",
    "    if s <= 0: \n",
    "        return 0.0\n",
    "    return float(vol[:k].sum() / s)\n",
    "\n",
    "def _unusual_bins_features(vp: pd.DataFrame, last_close: float, z_thresh: float = 2.0) -> dict:\n",
    "    \"\"\"\n",
    "    Summarize 'unusual' volume changes across bins (both increasing and decreasing).\n",
    "    Returns scalar features for the current window.\n",
    "    \"\"\"\n",
    "    if vp is None or len(vp) == 0 or not np.isfinite(last_close):\n",
    "        return {k: 0.0 for k in [\n",
    "            \"vp_unusual_inc_count\",\"vp_unusual_inc_maxz\",\"vp_unusual_inc_share\",\"vp_unusual_inc_near_px\",\n",
    "            \"vp_unusual_dec_count\",\"vp_unusual_dec_maxz\",\"vp_unusual_dec_share\",\"vp_unusual_dec_near_px\"\n",
    "        ]}\n",
    "\n",
    "    # sort by mid-price so \"neighbor\" means adjacent price bins\n",
    "    tmp = vp.copy()\n",
    "    tmp[\"mid\"] = (tmp[\"minPrice\"] + tmp[\"maxPrice\"]) * 0.5\n",
    "    tmp = tmp.sort_values(\"mid\").reset_index(drop=True)\n",
    "\n",
    "    diff = tmp[\"aggregateVolume\"].diff()\n",
    "    mu   = diff.rolling(10, min_periods=1).mean()\n",
    "    sd   = diff.rolling(10, min_periods=1).std()\n",
    "    z    = ((diff - mu) / sd.replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    inc_mask = (diff > 0) & (z >= z_thresh)\n",
    "    dec_mask = (diff < 0) & (z <= -z_thresh)\n",
    "\n",
    "    total_vol = float(tmp[\"aggregateVolume\"].sum()) if tmp[\"aggregateVolume\"].sum() > 0 else 1.0\n",
    "    inc_vol = float(tmp.loc[inc_mask, \"aggregateVolume\"].sum())\n",
    "    dec_vol = float(tmp.loc[dec_mask, \"aggregateVolume\"].sum())\n",
    "\n",
    "    # locate current price's bin\n",
    "    idx_px = np.searchsorted(tmp[\"maxPrice\"].to_numpy(), last_close)\n",
    "    idx_px = np.clip(idx_px, 0, len(tmp)-1)  # safe index\n",
    "\n",
    "    return {\n",
    "        \"vp_unusual_inc_count\": float(inc_mask.sum()),\n",
    "        \"vp_unusual_inc_maxz\": float(z[inc_mask].max() if inc_mask.any() else 0.0),\n",
    "        \"vp_unusual_inc_share\": float(inc_vol / total_vol),\n",
    "        \"vp_unusual_inc_near_px\": float(inc_mask.iloc[idx_px]) if len(tmp) else 0.0,\n",
    "\n",
    "        \"vp_unusual_dec_count\": float(dec_mask.sum()),\n",
    "        \"vp_unusual_dec_maxz\": float((-z[dec_mask]).max() if dec_mask.any() else 0.0),  # magnitude\n",
    "        \"vp_unusual_dec_share\": float(dec_vol / total_vol),\n",
    "        \"vp_unusual_dec_near_px\": float(dec_mask.iloc[idx_px]) if len(tmp) else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- single-window feature extractor ----------\n",
    "def _vp_features_for_window(\n",
    "    df_window: pd.DataFrame,\n",
    "    n_bins: int,\n",
    "    coverages: list[float],\n",
    "    topk: int,\n",
    "    atr_col: str | None = None  # <- This hint is misleading but left as-is\n",
    "):\n",
    "    feats = {}\n",
    "    try:\n",
    "        vp = getVPWithOHLC(df_window[[\"open\",\"high\",\"low\",\"close\",\"volume\"]], nBins=n_bins, tick=0.25)\n",
    "        if vp is None or len(vp) == 0:\n",
    "            return feats\n",
    "    except Exception:\n",
    "        return feats\n",
    "\n",
    "    ent = _profile_entropy(vp)\n",
    "    conc_k = _topk_share(vp, k=topk)\n",
    "    last_close = float(df_window[\"close\"].iloc[-1])\n",
    "    feats.update(_unusual_bins_features(vp, last_close, z_thresh=2.0))\n",
    "\n",
    "    # optional ATR (for normalising distances)\n",
    "    last_atr = None\n",
    "\n",
    "\n",
    "    # ------------------- FIX: robust ATR lookup -------------------\n",
    "    if atr_col is not None:\n",
    "        try:\n",
    "            # Get the index of the last row of the current window slice\n",
    "            last_index_for_window = df_window.index[-1]\n",
    "            \n",
    "            # Look up that index in the passed ATR Series\n",
    "            val = float(atr_col.loc[last_index_for_window])\n",
    "            last_atr = val if np.isfinite(val) and val > 0 else None\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            # KeyError: index not found in atr_col\n",
    "            # IndexError: df_window was empty\n",
    "            # TypeError: atr_col was not a Series (just in case)\n",
    "            last_atr = None\n",
    "    # ------------------- END OF FIX -------------------\n",
    "\n",
    "    for cov in coverages:\n",
    "        vpoc_mid, va_low, va_high = _value_area(vp, cov)\n",
    "\n",
    "        vpoc_dist = (last_close - vpoc_mid) if np.isfinite(vpoc_mid) else np.nan\n",
    "        vpoc_dist_pct = (vpoc_dist / last_close) if (np.isfinite(vpoc_dist) and last_close != 0) else np.nan\n",
    "        va_width = (va_high - va_low) if (np.isfinite(va_high) and np.isfinite(va_low)) else np.nan\n",
    "        va_width_pct = (va_width / last_close) if (np.isfinite(va_width) and last_close != 0) else np.nan\n",
    "\n",
    "        # New: inside-VA flag and location code\n",
    "        in_va = float(np.isfinite(va_low) and np.isfinite(va_high) and (va_low <= last_close <= va_high))\n",
    "        loc_code = (\n",
    "            -1.0 if (np.isfinite(va_low) and last_close < va_low) else\n",
    "             1.0 if (np.isfinite(va_high) and last_close > va_high) else\n",
    "             0.0 if in_va == 1.0 else np.nan\n",
    "        )\n",
    "\n",
    "        # Optional: distance normalised by ATR\n",
    "        vpoc_dist_atr = (vpoc_dist / last_atr) if (last_atr is not None and np.isfinite(vpoc_dist)) else np.nan\n",
    "\n",
    "        tag = f\"{int(cov*100)}\"  # \"40\" or \"70\"\n",
    "        feats.update({\n",
    "            f\"vp_vpoc_{tag}\": vpoc_mid,\n",
    "            f\"vp_val_{tag}\": va_low,\n",
    "            f\"vp_vah_{tag}\": va_high,\n",
    "            f\"vp_va_width_{tag}\": va_width,\n",
    "            f\"vp_va_width_pct_{tag}\": va_width_pct,\n",
    "            f\"vp_vpoc_dist_{tag}\": vpoc_dist,\n",
    "            f\"vp_vpoc_dist_pct_{tag}\": vpoc_dist_pct,\n",
    "            f\"vp_in_va_{tag}\": in_va,               # <- NEW\n",
    "            f\"vp_loc_{tag}\": loc_code,              # <- NEW (-1 below, 0 inside, +1 above)\n",
    "            f\"vp_vpoc_dist_atr_{tag}\": vpoc_dist_atr,  # <- NEW (if ATR provided)\n",
    "            f\"vp_above_vah_{tag}\": float(last_close > va_high) if np.isfinite(va_high) else 0.0,\n",
    "            f\"vp_below_val_{tag}\": float(last_close < va_low) if np.isfinite(va_low) else 0.0,\n",
    "        })\n",
    "\n",
    "    feats[\"vp_entropy\"] = ent\n",
    "    feats[\"vp_topk_share\"] = conc_k\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------- PARALLEL rolling builder (drop-in replacement) ----------\n",
    "def build_custom_vp_features(\n",
    "    df: pd.DataFrame,\n",
    "    windows: list[int] = (5, 10, 15, 30, 60),\n",
    "    coverages: list[float] = (0.40, 0.70),\n",
    "    n_bins: int = 24,\n",
    "    topk: int = 3,\n",
    "    stride: int = 1,            # compute every 'stride' bars; forward-fill in between\n",
    "    shift_by_one: bool = True,  # lag features one bar to avoid leakage\n",
    "    n_jobs: int = -1,           # NEW: parallel workers (-1 = all cores)\n",
    "    backend: str = \"loky\",      # NEW: \"loky\" (processes) or \"threading\" (no pickling of df)\n",
    "    batch_size: str | int = \"auto\"  # NEW: joblib batching\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rolling Volume-Profile features using custom getVPWithOHLC (no binary deps).\n",
    "    df must have: ['open','high','low','close','volume'] and DateTimeIndex.\n",
    "    Returns a DataFrame aligned with df.index.\n",
    "    Parallelized version ‚Äî identical outputs to the serial implementation.\n",
    "    \"\"\"\n",
    "    assert {\"open\",\"high\",\"low\",\"close\",\"volume\"}.issubset(df.columns)\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # tiny wrapper so each task only needs an index\n",
    "    def _compute_at_index(i: int, w: int):\n",
    "        # mirrors your exact slicing & call\n",
    "        sl = df.iloc[i + 1 - w : i + 1]\n",
    "        # This call is unchanged. 'df_volatility_features' is assumed\n",
    "        # to be in the scope where this function is defined.\n",
    "        return _vp_features_for_window(sl, n_bins=n_bins, coverages=list(coverages), topk=topk, atr_col=df_volatility_features['atr_14_rma'])\n",
    "\n",
    "    for w in windows:\n",
    "        # pre-allocate rows of dicts to keep positions identical\n",
    "        rows = [{} for _ in range(len(df))]\n",
    "        idxs = list(df.index)\n",
    "\n",
    "        # which indices actually compute features (same condition as before)\n",
    "        compute_idxs = [i for i in range(len(df)) if (i + 1) >= w and (i % stride) == 0]\n",
    "\n",
    "        if compute_idxs:\n",
    "            # run tasks in parallel\n",
    "            results = Parallel(n_jobs=n_jobs, backend=backend, batch_size=batch_size)(\n",
    "                delayed(_compute_at_index)(i, w) for i in compute_idxs\n",
    "            )\n",
    "            # place back into the pre-allocated list in the same positions\n",
    "            for i, feats in zip(compute_idxs, results):\n",
    "                rows[i] = feats\n",
    "\n",
    "        # forward-fill in between computed steps (same behavior)\n",
    "        feats_df = pd.DataFrame(rows, index=idxs).ffill()\n",
    "        feats_df = feats_df.add_suffix(f\"_w{w}\")  # same suffixing\n",
    "        out = pd.concat([out, feats_df], axis=1)\n",
    "\n",
    "    # numeric hygiene (unchanged)\n",
    "    out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "    # optional lag to prevent label leakage (unchanged)\n",
    "    if shift_by_one:\n",
    "        out = out.shift(1).fillna(0.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----------------- usage -----------------\n",
    "# This assumes 'df_filtered' and 'df_volatility_features' are defined\n",
    "df_vp = build_custom_vp_features(\n",
    "    df_filtered,\n",
    "    windows=[5, 10, 15, 30, 60],\n",
    "    coverages=[0.40, 0.70],\n",
    "    n_bins=24,\n",
    "    topk=3,\n",
    "    stride=5,            # speed-up knob\n",
    "    shift_by_one=True,   # good for ML\n",
    "    n_jobs=-1,           # <‚Äî parallel\n",
    "    backend=\"threading\"       # or \"threading\" to avoid pickling the df\n",
    ")\n",
    "\n",
    "\n",
    "assert len(df_vp) == len(df_filtered)\n",
    "df_vp.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675afa36",
   "metadata": {},
   "source": [
    "# üìä Volume Profile Feature Summary\n",
    "\n",
    "Each feature is computed per rolling window (`w ‚àà {5,10,15,30,60}`)  \n",
    "and receives a suffix `_{w}` ‚Äî e.g., `_w5`, `_w10`, `_w15`, `_w30`, `_w60`.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Coverage-Specific Features (for each coverage level `{40, 70}`)\n",
    "| Feature | Description |\n",
    "|----------|--------------|\n",
    "| **`vp_vpoc_{tag}`** | Volume Point of Control (VPOC) mid-price ‚Äî bin with maximum aggregate volume. |\n",
    "| **`vp_val_{tag}`** | Value Area Low (VAL) price boundary for coverage `{tag}`. |\n",
    "| **`vp_vah_{tag}`** | Value Area High (VAH) price boundary for coverage `{tag}`. |\n",
    "| **`vp_va_width_{tag}`** | Width of Value Area = `VAH ‚àí VAL` (in price units). |\n",
    "| **`vp_va_width_pct_{tag}`** | Value-area width as a fraction of last close (`va_width / last_close`). |\n",
    "| **`vp_vpoc_dist_{tag}`** | Distance from last close to VPOC (in price units). |\n",
    "| **`vp_vpoc_dist_pct_{tag}`** | Relative VPOC distance (`vpoc_dist / last_close`). |\n",
    "| **`vp_in_va_{tag}`** | 1 if last close ‚àà [VAL, VAH], else 0. |\n",
    "| **`vp_loc_{tag}`** | Location code of last close: ‚àí1 = below VAL, 0 = inside VA, +1 = above VAH. |\n",
    "| **`vp_vpoc_dist_atr_{tag}`** | VPOC distance normalised by ATR (if available). |\n",
    "| **`vp_above_vah_{tag}`** | 1 if last close > VAH, else 0. |\n",
    "| **`vp_below_val_{tag}`** | 1 if last close < VAL, else 0. |\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Unusual Volume-Change Features\n",
    "| Feature | Description |\n",
    "|----------|--------------|\n",
    "| **`vp_unusual_inc_count`** | Count of bins with unusually **increasing** volume (z ‚â• 2). |\n",
    "| **`vp_unusual_inc_maxz`** | Maximum z-score among increasing-volume bins. |\n",
    "| **`vp_unusual_inc_share`** | Share of total volume contained in increasing-volume bins. |\n",
    "| **`vp_unusual_inc_near_px`** | 1 if current price‚Äôs bin shows an increasing-volume anomaly. |\n",
    "| **`vp_unusual_dec_count`** | Count of bins with unusually **decreasing** volume (z ‚â§ ‚àí2). |\n",
    "| **`vp_unusual_dec_maxz`** | Maximum |z| among decreasing-volume bins. |\n",
    "| **`vp_unusual_dec_share`** | Share of total volume in decreasing-volume bins. |\n",
    "| **`vp_unusual_dec_near_px`** | 1 if current price‚Äôs bin shows a decreasing-volume anomaly. |\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Profile Shape & Concentration\n",
    "| Feature | Description |\n",
    "|----------|--------------|\n",
    "| **`vp_entropy`** | Entropy of the volume distribution (higher = more evenly spread). |\n",
    "| **`vp_topk_share`** | Fraction of total volume held in the top K bins (K = 3). |\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Example Naming by Window\n",
    "| Example | Meaning |\n",
    "|----------|----------|\n",
    "| `vp_vpoc_70_w30` | 70 % coverage VPOC on 30-bar window. |\n",
    "| `vp_in_va_40_w10` | Whether last close lies in 40 % value area on 10-bar window. |\n",
    "| `vp_unusual_inc_share_w60` | Share of increasing-volume bins on 60-bar window. |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Units & Conventions\n",
    "- **Price features:** raw price units (e.g., index points).  \n",
    "- **`*_pct` features:** relative to last close (e.g., 0.01 = 1 %).  \n",
    "- **Flags:** binary 0/1 values.  \n",
    "- **`vp_loc_*`:** ‚àí1 = below VAL, 0 = inside, +1 = above.  \n",
    "- **ATR-normalised:** measured in ‚ÄúATR units‚Äù (distance √∑ ATR).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb95e8d",
   "metadata": {},
   "source": [
    "### Seasonality & Time Encoding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ca2792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_time created with shape: (1691325, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tod_sin    0\n",
       "tod_cos    0\n",
       "dow_sin    0\n",
       "dow_cos    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seasonality & Time encoding (UTC)\n",
    "# ===============================\n",
    "\n",
    "def build_time_encodings_utc(\n",
    "    df: pd.DataFrame,\n",
    "    shift_by_one: bool = False   # usually not needed for time encodings\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create cyclic time features in UTC:\n",
    "      - tod_sin, tod_cos : time-of-day (0..24h) encoded on unit circle\n",
    "      - dow_sin, dow_cos : day-of-week (Mon=0..Sun=6) encoded on unit circle\n",
    "\n",
    "    Assumes df.index is a DateTimeIndex; if tz-naive, treats it as UTC.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"DataFrame index must be a pandas.DatetimeIndex.\")\n",
    "\n",
    "    # Ensure UTC\n",
    "    if df.index.tz is None:\n",
    "        idx_utc = df.index.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        idx_utc = df.index.tz_convert(\"UTC\")\n",
    "\n",
    "    # ---- time of day (fraction of day) ----\n",
    "    # use seconds for sub-minute timestamps as well\n",
    "    seconds = (\n",
    "        idx_utc.hour.astype(int) * 3600\n",
    "        + idx_utc.minute.astype(int) * 60\n",
    "        + idx_utc.second.astype(int)\n",
    "    )\n",
    "    # include microseconds if present\n",
    "    if hasattr(idx_utc, \"microsecond\"):\n",
    "        seconds = seconds + (idx_utc.microsecond.astype(int) / 1_000_000.0)\n",
    "\n",
    "    tod_frac = seconds / 86_400.0                        # in [0,1)\n",
    "    tod_ang  = 2.0 * np.pi * tod_frac\n",
    "    tod_sin  = np.sin(tod_ang)\n",
    "    tod_cos  = np.cos(tod_ang)\n",
    "\n",
    "    # ---- day of week (Mon=0..Sun=6) ----\n",
    "    dow = idx_utc.dayofweek.astype(float)               # 0..6\n",
    "    dow_ang = 2.0 * np.pi * (dow / 7.0)\n",
    "    dow_sin = np.sin(dow_ang)\n",
    "    dow_cos = np.cos(dow_ang)\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"tod_sin\": tod_sin,\n",
    "            \"tod_cos\": tod_cos,\n",
    "            \"dow_sin\": dow_sin,\n",
    "            \"dow_cos\": dow_cos,\n",
    "        },\n",
    "        index=df.index,  # keep original index object\n",
    "    ).astype(float)\n",
    "\n",
    "    if shift_by_one:\n",
    "        out = out.shift(1).fillna(0.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "df_time = build_time_encodings_utc(df_filtered, shift_by_one=False)\n",
    "assert len(df_time) == len(df_filtered)\n",
    "print(\"df_time created with shape:\", df_time.shape)\n",
    "df_time.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3abce7",
   "metadata": {},
   "source": [
    "### GARCH Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8713ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GARCH params/meta: {'p': 5, 'q': 5, 'criterion': 'bic', 'aic': 414513.2713824321, 'bic': 414625.53818153293, 'llf': -207245.63569121604, 'used_obs': 200000}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "garch_vol_best    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# GARCH Volatility (best (p,q))\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "from arch import arch_model\n",
    "\n",
    "# --------- helper: single (p,q) fit ----------\n",
    "def _fit_garch_single(\n",
    "    series: pd.Series,\n",
    "    p: int,\n",
    "    q: int,\n",
    "    mean: str = \"zero\",      # \"zero\" is good for diffs/returns\n",
    "    dist: str = \"normal\",    # \"normal\",\"t\",\"skewt\"\n",
    "    rescale: bool = True,    # let arch rescale internally\n",
    "    fit_options: dict | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit one GARCH(p,q); return dict with metrics and conditional volatility.\n",
    "    Returns None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        am = arch_model(series, vol=\"GARCH\", p=p, q=q, mean=mean, dist=dist, rescale=rescale)\n",
    "        # safer fit defaults\n",
    "        if fit_options is None:\n",
    "            fit_options = {\"disp\": \"off\", \"update_freq\": 0}\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            res = am.fit(**fit_options)\n",
    "\n",
    "        # Not all ARCH versions expose the same attrs ‚Äî use only robust ones\n",
    "        out = {\n",
    "            \"p\": int(p),\n",
    "            \"q\": int(q),\n",
    "            \"aic\": float(getattr(res, \"aic\", np.inf)),\n",
    "            \"bic\": float(getattr(res, \"bic\", np.inf)),\n",
    "            \"llf\": float(getattr(res, \"loglikelihood\", np.nan)),\n",
    "            \"cond_vol\": pd.Series(res.conditional_volatility, index=series.index).astype(float)\n",
    "        }\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --------- grid search + choose best ----------\n",
    "def garch_grid_best(\n",
    "    series: pd.Series,\n",
    "    p_range=range(1, 6),\n",
    "    q_range=range(1, 6),\n",
    "    criterion: str = \"bic\",       # \"bic\", \"aic\" (min) or \"llf\" (max)\n",
    "    mean: str = \"zero\",\n",
    "    dist: str = \"normal\",\n",
    "    rescale: bool = True,\n",
    "    n_jobs: int = -1,\n",
    "    backend: str = \"loky\",        # or \"threading\" on macOS if needed\n",
    "    shift_by_one: bool = True,    # avoid leakage\n",
    "    fill_value: float = 0.0,\n",
    "    max_obs: int | None = 200_000,  # optionally limit obs for speed\n",
    "    winsorize: bool = True,       # trim outliers for stability\n",
    "    winsor_q: float = 0.001,      # 0.1% tails\n",
    "    std_input: bool = True,       # standardize for numeric stability\n",
    "    fit_options: dict | None = None\n",
    ") -> tuple[pd.Series, dict]:\n",
    "    \"\"\"\n",
    "    Fit GARCH(p,q) in parallel across p_range x q_range.\n",
    "    Select best model by `criterion` and return (best_vol_series, best_meta).\n",
    "    \"\"\"\n",
    "    s = pd.Series(series).astype(float).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # Optional subsample from the tail to speed up\n",
    "    if max_obs is not None and len(s) > max_obs:\n",
    "        s_fit = s.iloc[-max_obs:].copy()\n",
    "    else:\n",
    "        s_fit = s.copy()\n",
    "\n",
    "    # Winsorize (trim extreme tails) to reduce optimizer issues\n",
    "    if winsorize and len(s_fit) > 10:\n",
    "        lo = s_fit.quantile(winsor_q)\n",
    "        hi = s_fit.quantile(1 - winsor_q)\n",
    "        s_fit = s_fit.clip(lower=lo, upper=hi)\n",
    "\n",
    "    # Standardize (improves optimizer stability); keep scale to rescale back\n",
    "    mean_s, std_s = (0.0, 1.0)\n",
    "    if std_input and s_fit.std() > 0:\n",
    "        mean_s = float(s_fit.mean())\n",
    "        std_s = float(s_fit.std(ddof=0))\n",
    "        s_fit = (s_fit - mean_s) / std_s\n",
    "\n",
    "    # If variance is zero after cleaning, return flat series\n",
    "    if s_fit.var(ddof=0) == 0 or not np.isfinite(s_fit.var(ddof=0)):\n",
    "        best = pd.Series(fill_value, index=series.index)\n",
    "        meta = {\"p\": 1, \"q\": 1, \"criterion\": criterion, \"aic\": np.inf, \"bic\": np.inf, \"llf\": np.nan}\n",
    "        return (best.shift(1).fillna(fill_value) if shift_by_one else best, meta)\n",
    "\n",
    "    # Parallel fits\n",
    "    jobs = (\n",
    "        delayed(_fit_garch_single)(s_fit, p, q, mean, dist, rescale, fit_options)\n",
    "        for p in p_range for q in q_range\n",
    "    )\n",
    "    results = Parallel(n_jobs=n_jobs, backend=backend, verbose=0)(jobs)\n",
    "    results = [r for r in results if r is not None and np.isfinite(r.get(criterion.lower(), np.inf) if criterion!=\"llf\" else r.get(\"llf\", -np.inf))]\n",
    "\n",
    "    if not results:\n",
    "        # Fallback: flat series\n",
    "        best = pd.Series(fill_value, index=series.index)\n",
    "        meta = {\"p\": None, \"q\": None, \"criterion\": criterion, \"aic\": np.inf, \"bic\": np.inf, \"llf\": np.nan}\n",
    "        return (best.shift(1).fillna(fill_value) if shift_by_one else best, meta)\n",
    "\n",
    "    # Choose best by criterion\n",
    "    crit = criterion.lower()\n",
    "    if crit in (\"bic\", \"aic\"):\n",
    "        best_res = min(results, key=lambda r: r[crit])\n",
    "    elif crit == \"llf\":\n",
    "        best_res = max(results, key=lambda r: r[\"llf\"])\n",
    "    else:\n",
    "        raise ValueError(\"criterion must be one of: 'bic','aic','llf'\")\n",
    "\n",
    "    # Take conditional volatility from the fit sample and align to full index\n",
    "    vol_fit = best_res[\"cond_vol\"]\n",
    "    # If standardized input, rescale volatility to original units (vol scales by std)\n",
    "    if std_input:\n",
    "        vol_fit = vol_fit * std_s\n",
    "\n",
    "    vol = vol_fit.reindex(series.index).ffill().fillna(fill_value)\n",
    "    if shift_by_one:\n",
    "        vol = vol.shift(1).fillna(fill_value)\n",
    "\n",
    "    meta = {\n",
    "        \"p\": best_res[\"p\"],\n",
    "        \"q\": best_res[\"q\"],\n",
    "        \"criterion\": criterion,\n",
    "        \"aic\": best_res[\"aic\"],\n",
    "        \"bic\": best_res[\"bic\"],\n",
    "        \"llf\": best_res[\"llf\"],\n",
    "        \"used_obs\": int(len(s_fit))\n",
    "    }\n",
    "    return vol.astype(float), meta\n",
    "\n",
    "\n",
    "# --------- convenience wrapper: build feature from prices ----------\n",
    "def build_garch_vol_feature_from_prices(\n",
    "    df: pd.DataFrame,\n",
    "    price_col: str = \"close\",\n",
    "    diff_kind: str = \"diff\",      # \"diff\" or \"logret\"\n",
    "    **grid_kwargs\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Make stationary series from prices, run GARCH grid, return\n",
    "    (DataFrame with 'garch_vol_best', meta).\n",
    "    \"\"\"\n",
    "    prices = df[price_col].astype(float)\n",
    "    if diff_kind == \"diff\":\n",
    "        x = prices.diff()\n",
    "    elif diff_kind == \"logret\":\n",
    "        x = np.log(prices) - np.log(prices.shift(1))\n",
    "    else:\n",
    "        raise ValueError(\"diff_kind must be 'diff' or 'logret'\")\n",
    "\n",
    "    x = x.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    vol, meta = garch_grid_best(\n",
    "        x,\n",
    "        **grid_kwargs\n",
    "    )\n",
    "    df_feature = pd.DataFrame({\"garch_vol_best\": vol}, index=df.index)\n",
    "    df_feature = df_feature.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    return df_feature, meta\n",
    "\n",
    "\n",
    "# --------- usage with your df_filtered ----------\n",
    "# Stationary input: use close price differences\n",
    "# (You can switch backend to \"threading\" on macOS if loky pickling is slow)\n",
    "df_garch_feature, meta = build_garch_vol_feature_from_prices(\n",
    "    df_filtered,\n",
    "    price_col=\"close\",\n",
    "    diff_kind=\"diff\",\n",
    "    p_range=range(1, 6),\n",
    "    q_range=range(1, 6),\n",
    "    criterion=\"bic\",        # or \"aic\" / \"llf\"\n",
    "    mean=\"zero\",\n",
    "    dist=\"normal\",\n",
    "    rescale=True,\n",
    "    n_jobs=-1,\n",
    "    backend=\"loky\",\n",
    "    shift_by_one=True,\n",
    "    max_obs=200_000,        # speed knob\n",
    "    winsorize=True,\n",
    "    std_input=True,\n",
    "    fit_options={\"disp\": \"off\", \"update_freq\": 0}  # you can add {\"tol\":1e-6,\"options\":{\"maxiter\":3000}}\n",
    ")\n",
    "\n",
    "print(\"Best GARCH params/meta:\", meta)\n",
    "assert len(df_garch_feature) == len(df_filtered)\n",
    "\n",
    "df_garch_feature.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a430bc",
   "metadata": {},
   "source": [
    "### Candlestick Pattern Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2e249a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_cdl created with shape: (1691325, 235)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cdlengulfing_raw                   0\n",
       "cdlhammer_raw                      0\n",
       "cdlshootingstar_raw                0\n",
       "cdldoji_raw                        0\n",
       "cdlpiercing_raw                    0\n",
       "                                  ..\n",
       "cdl3whitesoldiers_sign_bear_w60    0\n",
       "cdl3blackcrows_sign_bear_w60       0\n",
       "bull_strength_w60                  0\n",
       "bear_strength_w60                  0\n",
       "net_strength_w60                   0\n",
       "Length: 235, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- FAST Candlestick Pattern Features (batched, low-fragmentation) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_candlestick_features_fast(\n",
    "    df: pd.DataFrame,\n",
    "    windows: tuple[int, ...] = (5, 10, 15, 30, 60),\n",
    "    shift_by_one: bool = True,\n",
    "    normalize: bool = True,\n",
    "    float_dtype: str = \"float32\",\n",
    ") -> pd.DataFrame:\n",
    "    need = {\"open\",\"high\",\"low\",\"close\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {need}\")\n",
    "\n",
    "    O = df[\"open\"].astype(float).to_numpy()\n",
    "    H = df[\"high\"].astype(float).to_numpy()\n",
    "    L = df[\"low\"].astype(float).to_numpy()\n",
    "    C = df[\"close\"].astype(float).to_numpy()\n",
    "    idx = df.index\n",
    "\n",
    "    try:\n",
    "        import talib as ta\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"TA-Lib is required. Install with `pip install TA-Lib` \"\n",
    "            \"(may need system libs on macOS/Linux).\"\n",
    "        ) from e\n",
    "\n",
    "    patterns = [\n",
    "        \"CDLENGULFING\",\"CDLHAMMER\",\"CDLSHOOTINGSTAR\",\"CDLDOJI\",\n",
    "        \"CDLPIERCING\",\"CDLDARKCLOUDCOVER\",\"CDLMORNINGSTAR\",\"CDLEVENINGSTAR\",\n",
    "        \"CDL3WHITESOLDIERS\",\"CDL3BLACKCROWS\",\n",
    "    ]\n",
    "\n",
    "    # ---- raw & sign frames (built once, no per-column insert) ----\n",
    "    raw_map = {}\n",
    "    sign_map = {}\n",
    "    for name in patterns:\n",
    "        fn = getattr(ta, name)\n",
    "        raw = fn(O, H, L, C).astype(np.int16)  # TA-Lib returns -100..100\n",
    "        col_raw = f\"{name.lower()}_raw\"\n",
    "        raw_map[col_raw] = raw\n",
    "\n",
    "        if normalize:\n",
    "            sign = (raw // 100).clip(-1, 1).astype(np.int8)\n",
    "        else:\n",
    "            sign = np.sign(raw).astype(np.int8)\n",
    "        col_sign = f\"{name.lower()}_sign\"\n",
    "        sign_map[col_sign] = sign\n",
    "\n",
    "    raw_df  = pd.DataFrame(raw_map, index=idx)\n",
    "    sign_df = pd.DataFrame(sign_map, index=idx)\n",
    "\n",
    "    # Precompute boolean/int mats for rolling\n",
    "    nz_df   = (sign_df != 0).astype(np.int8)\n",
    "    bull_df = (sign_df == 1).astype(np.int8)\n",
    "    bear_df = (sign_df == -1).astype(np.int8)\n",
    "\n",
    "    # ---- rolling windows (batch ops), collect pieces, concat once ----\n",
    "    pieces = [raw_df, sign_df]\n",
    "    P = sign_df.shape[1]\n",
    "\n",
    "    for w in windows:\n",
    "        # presence / count\n",
    "        any_df   = nz_df.rolling(w, min_periods=1).max().astype(np.int8)\n",
    "        cnt_df   = nz_df.rolling(w, min_periods=1).sum().astype(np.int16)\n",
    "        bullf_df = bull_df.rolling(w, min_periods=1).mean().astype(float_dtype)\n",
    "        bearf_df = bear_df.rolling(w, min_periods=1).mean().astype(float_dtype)\n",
    "\n",
    "        any_df.columns   = [f\"{c}_any_w{w}\"   for c in sign_df.columns]\n",
    "        cnt_df.columns   = [f\"{c}_count_w{w}\" for c in sign_df.columns]\n",
    "        bullf_df.columns = [f\"{c}_bull_w{w}\"  for c in sign_df.columns]\n",
    "        bearf_df.columns = [f\"{c}_bear_w{w}\"  for c in sign_df.columns]\n",
    "        pieces.extend([any_df, cnt_df, bullf_df, bearf_df])\n",
    "\n",
    "        # aggregated strengths across patterns (single Series each)\n",
    "        bull_strength = bull_df.rolling(w, min_periods=1).sum().sum(axis=1) / float(w * P)\n",
    "        bear_strength = bear_df.rolling(w, min_periods=1).sum().sum(axis=1) / float(w * P)\n",
    "        net_strength  = bull_strength - bear_strength\n",
    "\n",
    "        pieces.append(bull_strength.astype(float_dtype).rename(f\"bull_strength_w{w}\").to_frame())\n",
    "        pieces.append(bear_strength.astype(float_dtype).rename(f\"bear_strength_w{w}\").to_frame())\n",
    "        pieces.append(net_strength.astype(float_dtype).rename(f\"net_strength_w{w}\").to_frame())\n",
    "\n",
    "    out = pd.concat(pieces, axis=1, copy=False)\n",
    "    out = out.replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "    if shift_by_one:\n",
    "        out = out.shift(1)\n",
    "\n",
    "    out = out.fillna(0.0)\n",
    "    # downcast floats to save RAM\n",
    "    for c in out.columns:\n",
    "        if pd.api.types.is_float_dtype(out[c]):\n",
    "            out[c] = out[c].astype(float_dtype)\n",
    "\n",
    "    # ensure no dup col names\n",
    "    out = out.loc[:, ~out.columns.duplicated()].copy()\n",
    "    return out\n",
    "\n",
    "# ---------- usage ----------\n",
    "df_cdl = build_candlestick_features_fast(\n",
    "    df_filtered[[\"open\",\"high\",\"low\",\"close\"]],\n",
    "    windows=(5, 10, 15, 30, 60),\n",
    "    shift_by_one=True,\n",
    "    normalize=True,\n",
    ")\n",
    "print(\"df_cdl created with shape:\", df_cdl.shape)\n",
    "assert len(df_cdl) == len(df_filtered)\n",
    "df_cdl.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707da0cc",
   "metadata": {},
   "source": [
    "### Combine features to a DataFrame and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75782ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultimate shape: (1691325, 727)\n"
     ]
    }
   ],
   "source": [
    "# ---- pick the canonical index (your price DF) ----\n",
    "base = df_filtered  # or whatever your master index DF is\n",
    "idx = base.index\n",
    "\n",
    "# ---- put every feature block in a list in one place ----\n",
    "blocks = [\n",
    "    df_norm_zscore,\n",
    "    df_returns_variants,\n",
    "    df_volatility_features,\n",
    "    df_momentum_features,\n",
    "    df_vol_z,\n",
    "    df_vwap_features,\n",
    "    df_microstructure,\n",
    "    df_vp,\n",
    "    df_time,\n",
    "    df_garch_feature,\n",
    "    df_cdl,\n",
    "]\n",
    "\n",
    "# optional: quick sanity checks before concat\n",
    "for i, b in enumerate(blocks, 1):\n",
    "    assert len(b) == len(idx), f\"Block {i} length {len(b)} != base length {len(idx)}\"\n",
    "    # If you‚Äôre not 100% sure indices match exactly, force alignment:\n",
    "    if not b.index.equals(idx):\n",
    "        blocks[i-1] = b.reindex(idx)\n",
    "\n",
    "# ---- single concat to avoid fragmentation ----\n",
    "df_ultimate_feature_set = pd.concat(blocks, axis=1, copy=False)\n",
    "\n",
    "# If you want to keep the index name/column from your base:\n",
    "df_ultimate_feature_set = df_ultimate_feature_set.reindex(index=idx)\n",
    "\n",
    "# OPTIONAL: if you‚Äôve decided to zero-fill remaining NaNs\n",
    "df_ultimate_feature_set = df_ultimate_feature_set.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "\n",
    "print(\"ultimate shape:\", df_ultimate_feature_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85204be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet (recommended)\n",
    "df_ultimate_feature_set.to_parquet(\n",
    "    \"ultimate_feature_set.parquet\",\n",
    "    engine=\"fastparquet\",          # or \"fastparquet\"\n",
    "    compression=\"zstd\",        # or \"snappy\"\n",
    "    index=True\n",
    ")\n",
    "\n",
    "# If you still want CSV too\n",
    "df_ultimate_feature_set.to_csv(\"ultimate_feature_set.csv\", index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
